"""Handles user input & triggers RAG calls."""
from __future__ import annotations

from datetime import datetime
import logging
from typing import Any, Dict

import streamlit as st

from frontend.ui.core.config import Constants
from frontend.ui.utils.error_handler import ErrorContext
from frontend.ui.components.common import ErrorDisplay
from frontend.ui.utils.streamlit_helpers import rerun

from .history import ChatHistory
from .utils import get_model_settings, check_model_availability, show_simple_loading, clear_loading

try:
    from frontend.ui.utils.model_manager import ModelManager  # type: ignore
    HAS_MODEL_MANAGER = True
except ImportError:
    ModelManager = None  # type: ignore
    HAS_MODEL_MANAGER = False

logger = logging.getLogger(__name__)


class ChatInputHandler:
    """Encapsulates *handle_chat_input* logic for composability."""

    def __init__(self, api_client):
        self.api_client = api_client
        self.history = ChatHistory()

    # --------------------------------------------------------------
    # Public entry point
    # --------------------------------------------------------------
    def render_input(self) -> bool:
        """Render Streamlit chatâ€‘input widget, returns *True* if interaction occurred."""
        ok, err_msg = check_model_availability(self.api_client)
        if not ok:
            ErrorDisplay.render_error_with_suggestions(
                err_msg,
                [
                    "ì„¤ì • í˜ì´ì§€ì—ì„œ ëª¨ë¸ì„ ì„ íƒí•˜ì„¸ìš”",
                    "Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”",
                    "ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”",
                ],
            )
            return False

        if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
            # --- user message immediately added
            self.history.add("user", prompt)

            # --- obtain fresh settings
            with ErrorContext("ëª¨ë¸ ì„¤ì • ì¡°íšŒ") as ctx:
                try:
                    settings = get_model_settings()
                except Exception as exc:
                    ctx.add_error(exc)
                    return False

            # --- reâ€‘check model availability just before API call
            re_ok, re_err = check_model_availability(self.api_client)
            if not re_ok:
                ErrorDisplay.render_error_with_suggestions(re_err, ["ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”"])
                return False

            # --- call backend (sync for now)
            with st.chat_message("assistant"):
                answer_placeholder = st.empty()
                status = st.empty()

                show_simple_loading(f"ğŸ¤– '{settings['model']}'ìœ¼ë¡œ ë‹µë³€ ìƒì„± ì¤‘...", status)

                with ErrorContext("RAG ë‹µë³€ ìƒì„±") as ctx:
                    try:
                        params = self._compose_params(prompt, settings)
                        result = self.api_client.generate_answer(**params)  # type: ignore[arg-type]

                        clear_loading(status)

                        if "error" in result:
                            self._handle_error(result["error"], settings["model"], answer_placeholder)
                            return True

                        answer = result.get("answer", "ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        answer_placeholder.markdown(answer)
                        self.history.add(
                            "assistant",
                            answer,
                            model_used=settings["model"],
                            sources=result.get("sources", []),
                            search_info=result.get("search_info", {}),
                            settings_used={
                                "temperature": settings["temperature"],
                                "top_k": settings["rag_top_k"],
                                "min_similarity": settings["min_similarity"],
                                "search_type": settings["search_type"],
                            },
                        )
                    except Exception as exc:
                        clear_loading(status)
                        answer_placeholder.empty()
                        ctx.add_error(exc)
                        model = 'gemma3n:latest'
                        # settings["model"]
                        self._handle_error(str(exc), model, answer_placeholder)

            rerun()  # immediate UI refresh so that new messages appear
            return True
        return False

    # --------------------------------------------------------------
    # Internals
    # --------------------------------------------------------------
    def _compose_params(self, prompt: str, settings: Dict[str, Any]):
        if HAS_MODEL_MANAGER:
            base = {"query": prompt}
            return ModelManager.apply_to_api_request(base)
        return {
            "query": prompt,
            "model": settings["model"],
            "temperature": settings["temperature"],
            "system_prompt": settings["system_prompt"],
            "top_k": settings["rag_top_k"],
            "min_score": settings["min_similarity"],
            "search_type": settings["search_type"],
            "timeout": settings["rag_timeout"],
        }

    def _handle_error(self, error_msg: str, model: str, placeholder):
        from frontend.ui.utils.error_handler import error_handler, GTRagError, ErrorType, ErrorSeverity

        placeholder.empty()

        # ì—ëŸ¬ ë©”ì‹œì§€ë¡œë¶€í„° Exception ê°ì²´ ìƒì„±
        try:
            # ëª¨ë¸ ê´€ë ¨ ì—ëŸ¬ì¸ì§€ í™•ì¸
            if "model" in error_msg.lower() or model:
                error_exception = GTRagError(
                    f"ëª¨ë¸ '{model}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {error_msg}",
                    error_type=ErrorType.MODEL_CONFIG,
                    severity=ErrorSeverity.MEDIUM,
                    suggestions=[
                        "ì„¤ì • í˜ì´ì§€ì—ì„œ ëª¨ë¸ì„ ë‹¤ì‹œ ì„ íƒí•˜ì„¸ìš”",
                        "Ollama ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•˜ì„¸ìš”",
                        "ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”"
                    ]
                )
            else:
                # ì¼ë°˜ì ì¸ API ì˜¤ë¥˜
                error_exception = Exception(error_msg)

            # í†µí•© ì—ëŸ¬ í•¸ë“¤ëŸ¬ ì‚¬ìš©
            error_handler.handle_error(error_exception, f"ëª¨ë¸ '{model}' API í˜¸ì¶œ")

        except Exception as e:
            # ì—ëŸ¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš° ê¸°ë³¸ í‘œì‹œ
            st.error(f"âŒ ì˜¤ë¥˜: {error_msg}")
            if model:
                st.info(f"ì‚¬ìš©ëœ ëª¨ë¸: {model}")
            st.info("ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”.")
