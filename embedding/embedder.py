"""
ì„ë² ë”© ëª¨ë“ˆ - ì§€ì—° ë¡œë”© ë° ìºì‹œ ê°œì„  ë²„ì „
"""
from sentence_transformers import SentenceTransformer
import numpy as np
import os
import logging
from typing import List, Optional
import functools

logger = logging.getLogger(__name__)

# ëª¨ë¸ ì „ì—­ ë³€ìˆ˜ (ì§€ì—° ë¡œë”©)
_model: Optional[SentenceTransformer] = None
_model_loading = False

# í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì •
MODEL_NAME = os.getenv("EMBEDDING_MODEL", "intfloat/multilingual-e5-large-instruct")
BATCH_SIZE = int(os.getenv("EMBEDDING_BATCH_SIZE", "32"))


def get_model() -> SentenceTransformer:
    """
    ì„ë² ë”© ëª¨ë¸ì„ ì§€ì—° ë¡œë”© ë°©ì‹ìœ¼ë¡œ ê°€ì ¸ì˜´
    ìµœì´ˆ í˜¸ì¶œ ì‹œì—ë§Œ ëª¨ë¸ì„ ë¡œë“œí•˜ê³ , ì´í›„ì—ëŠ” ìºì‹œëœ ì¸ìŠ¤í„´ìŠ¤ ì‚¬ìš©
    """
    global _model, _model_loading

    if _model is not None:
        return _model

    if _model_loading:
        # ë‹¤ë¥¸ ìŠ¤ë ˆë“œì—ì„œ ë¡œë”© ì¤‘ì¸ ê²½ìš° ëŒ€ê¸°
        import time
        while _model_loading and _model is None:
            time.sleep(0.1)
        if _model is not None:
            return _model

    try:
        _model_loading = True
        logger.info(f"ğŸ”„ Loading embedding model: {MODEL_NAME}")

        # ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
        cache_dir = os.getenv("HF_HOME", "/root/.cache/huggingface")

        # ëª¨ë¸ ë¡œë”© (ìºì‹œ í™œìš©)
        _model = SentenceTransformer(
            MODEL_NAME,
            cache_folder=cache_dir,
            device='cpu'  # GPUê°€ ìˆë‹¤ë©´ 'cuda'ë¡œ ë³€ê²½
        )

        logger.info(f"âœ… Embedding model loaded successfully")
        logger.info(f"ğŸ“Š Model info: {_model.get_sentence_embedding_dimension()} dimensions")

        return _model

    except Exception as e:
        logger.error(f"âŒ Failed to load embedding model: {e}")
        raise RuntimeError(f"Embedding model loading failed: {e}")
    finally:
        _model_loading = False


@functools.lru_cache(maxsize=1000)
def _cached_embed_single(text: str, prefix: str = "query") -> tuple:
    """
    ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„± (ìºì‹œ ì ìš©)
    tupleë¡œ ë°˜í™˜í•˜ì—¬ hashableí•˜ê²Œ ë§Œë“¦
    """
    model = get_model()

    # í”„ë¦¬í”½ìŠ¤ ì ìš©
    if prefix and not text.startswith(prefix):
        text = f"{prefix}: {text}"

    # ì„ë² ë”© ìƒì„±
    embedding = model.encode(
        text,
        convert_to_numpy=True,
        normalize_embeddings=True,
        show_progress_bar=False
    )

    return tuple(embedding.tolist())


def embed_texts(texts: List[str], prefix: str = "query") -> np.ndarray:
    """
    í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜

    Args:
        texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        prefix: í…ìŠ¤íŠ¸ ì•ì— ë¶™ì¼ í”„ë¦¬í”½ìŠ¤ (E5 ëª¨ë¸ìš©)

    Returns:
        ì •ê·œí™”ëœ ì„ë² ë”© ë²¡í„° ë°°ì—´
    """
    if not texts:
        return np.array([])

    try:
        # ë‹¨ì¼ í…ìŠ¤íŠ¸ì¸ ê²½ìš° ìºì‹œ í™œìš©
        if len(texts) == 1:
            cached_result = _cached_embed_single(texts[0], prefix)
            return np.array([list(cached_result)])

        # ë‹¤ì¤‘ í…ìŠ¤íŠ¸ì¸ ê²½ìš° ë°°ì¹˜ ì²˜ë¦¬
        model = get_model()

        # í”„ë¦¬í”½ìŠ¤ ì ìš©
        prefixed_texts = []
        for text in texts:
            if prefix and not text.startswith(prefix):
                prefixed_texts.append(f"{prefix}: {text}")
            else:
                prefixed_texts.append(text)

        logger.debug(f"Embedding {len(texts)} texts with batch_size={BATCH_SIZE}")

        # ë°°ì¹˜ ì„ë² ë”© ìƒì„±
        embeddings = model.encode(
            prefixed_texts,
            batch_size=BATCH_SIZE,
            convert_to_numpy=True,
            normalize_embeddings=True,
            show_progress_bar=False
        )

        logger.debug(f"Generated embeddings shape: {embeddings.shape}")
        return embeddings

    except Exception as e:
        logger.error(f"Embedding generation failed: {e}")
        raise RuntimeError(f"Failed to generate embeddings: {e}")


def warmup_model():
    """
    ëª¨ë¸ ì›Œë°ì—… - ì‹œìŠ¤í…œ ì‹œì‘ ì‹œ ë¯¸ë¦¬ í˜¸ì¶œí•˜ì—¬ ì´ˆê¸°í™”
    """
    try:
        logger.info("ğŸ”¥ Warming up embedding model...")
        model = get_model()

        # í…ŒìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
        test_embedding = embed_texts(["test embedding"])
        logger.info(f"âœ… Model warmup successful - shape: {test_embedding.shape}")

    except Exception as e:
        logger.error(f"âŒ Model warmup failed: {e}")
        raise


def get_model_info() -> dict:
    """
    ëª¨ë¸ ì •ë³´ ë°˜í™˜
    """
    try:
        model = get_model()
        return {
            "model_name": MODEL_NAME,
            "dimensions": model.get_sentence_embedding_dimension(),
            "max_sequence_length": getattr(model, 'max_seq_length', 'unknown'),
            "device": str(model.device),
            "is_loaded": _model is not None
        }
    except Exception as e:
        return {
            "model_name": MODEL_NAME,
            "error": str(e),
            "is_loaded": False
        }


def clear_cache():
    """
    ì„ë² ë”© ìºì‹œ ì´ˆê¸°í™”
    """
    _cached_embed_single.cache_clear()
    logger.info("Embedding cache cleared")


# ëª¨ë“ˆ ë¡œë“œ ì‹œ ì¦‰ì‹œ ì‹¤í–‰ë˜ì§€ ì•Šë„ë¡ í•¨ìˆ˜ë¡œ ë˜í•‘
def preload_model_if_needed():
    """
    í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ëœ ê²½ìš°ì—ë§Œ ëª¨ë¸ ì‚¬ì „ ë¡œë”©
    """
    if os.getenv("PRELOAD_EMBEDDING_MODEL", "false").lower() == "true":
        try:
            warmup_model()
        except Exception as e:
            logger.warning(f"Model preloading failed: {e}")


# í™˜ê²½ë³€ìˆ˜ ê¸°ë°˜ ì‚¬ì „ ë¡œë”©
if __name__ == "__main__":
    # ì§ì ‘ ì‹¤í–‰ ì‹œ ì›Œë°ì—… ìˆ˜í–‰
    warmup_model()
else:
    # ëª¨ë“ˆ import ì‹œ ì¡°ê±´ë¶€ ì‚¬ì „ ë¡œë”©
    preload_model_if_needed()