"""
ì„ë² ë”© ëª¨ë“ˆ - macOS/conda í™˜ê²½ ìµœì í™” ë²„ì „ (ìˆ˜ì •ë¨)
Sentence Transformersë¥¼ ì‚¬ìš©í•œ ë‹¤êµ­ì–´ ì„ë² ë”© ìƒì„±
"""
import os
import sys
import logging
from pathlib import Path
from typing import List, Optional
import numpy as np
from functools import lru_cache

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

# í™˜ê²½ë³€ìˆ˜ ì„¤ì • (ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ê²½ë¡œ ì§€ì •)
def setup_cache_directories():
    """ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •"""
    # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ìœ¼ë¡œ ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
    project_root = Path(__file__).parent.parent
    cache_root = project_root / ".cache"

    # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    cache_root.mkdir(exist_ok=True)

    # í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    os.environ["TRANSFORMERS_CACHE"] = str(cache_root / "transformers")
    os.environ["HF_HOME"] = str(cache_root / "huggingface")
    os.environ["TORCH_HOME"] = str(cache_root / "torch")
    os.environ["SENTENCE_TRANSFORMERS_HOME"] = str(cache_root / "sentence_transformers")

    # ë””ë ‰í† ë¦¬ ìƒì„±
    for env_var in ["TRANSFORMERS_CACHE", "HF_HOME", "TORCH_HOME", "SENTENCE_TRANSFORMERS_HOME"]:
        Path(os.environ[env_var]).mkdir(parents=True, exist_ok=True)

    logger.info(f"Cache directories set up in: {cache_root}")
    return cache_root

# ìºì‹œ ë””ë ‰í† ë¦¬ ì´ˆê¸° ì„¤ì •
try:
    setup_cache_directories()
except Exception as e:
    logger.warning(f"Failed to setup cache directories: {e}")

# ì´ì œ sentence_transformers import
try:
    from sentence_transformers import SentenceTransformer
except ImportError as e:
    logger.error(f"Failed to import sentence_transformers: {e}")
    raise

# ê¸°ë³¸ ëª¨ë¸ ì„¤ì •
DEFAULT_MODEL = "intfloat/multilingual-e5-large-instruct"
FALLBACK_MODELS = [
    "intfloat/multilingual-e5-base",
    "intfloat/e5-large-v2",
    "sentence-transformers/all-MiniLM-L6-v2"
]

# ì „ì—­ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤
_model_instance: Optional[SentenceTransformer] = None
_model_lock = False


def get_model_name(model: SentenceTransformer) -> str:
    """ëª¨ë¸ ì´ë¦„ì„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°"""
    # ì—¬ëŸ¬ ê°€ëŠ¥í•œ ì†ì„±ë“¤ì„ ì‹œë„
    for attr in ['model_name', '_model_name', 'model_name_or_path', '_model_name_or_path']:
        if hasattr(model, attr):
            value = getattr(model, attr)
            if value:
                return str(value)

    # ëª¨ë¸ ì„¤ì •ì—ì„œ ì°¾ê¸° ì‹œë„
    if hasattr(model, '_modules') and hasattr(model._modules, '0'):
        first_module = model._modules['0']
        if hasattr(first_module, 'auto_model') and hasattr(first_module.auto_model, 'config'):
            config = first_module.auto_model.config
            if hasattr(config, '_name_or_path'):
                return str(config._name_or_path)
            if hasattr(config, 'name_or_path'):
                return str(config.name_or_path)

    # ê¸°ë³¸ê°’ ë°˜í™˜
    return "unknown"


@lru_cache(maxsize=1)
def get_model(model_name: str = DEFAULT_MODEL) -> SentenceTransformer:
    """
    ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ìºì‹œëœ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜)

    Args:
        model_name: ì‚¬ìš©í•  ëª¨ë¸ëª…

    Returns:
        SentenceTransformer ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤

    Raises:
        RuntimeError: ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ
    """
    global _model_instance, _model_lock

    if _model_instance is not None:
        return _model_instance

    if _model_lock:
        # ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë¡œë”© ì¤‘ì¸ ê²½ìš° ëŒ€ê¸°
        import time
        for _ in range(30):  # ìµœëŒ€ 30ì´ˆ ëŒ€ê¸°
            time.sleep(1)
            if _model_instance is not None:
                return _model_instance
        raise RuntimeError("Model loading timeout")

    _model_lock = True

    try:
        logger.info(f"Loading embedding model: {model_name}")

        # ëª¨ë¸ ë¡œë“œ ì‹œë„
        models_to_try = [model_name] + FALLBACK_MODELS

        for idx, model_to_load in enumerate(models_to_try):
            try:
                logger.info(f"Attempting to load model {idx+1}/{len(models_to_try)}: {model_to_load}")

                # ëª¨ë¸ ë¡œë“œ ì˜µì…˜
                model_kwargs = {
                    'device': 'cpu',  # CPU ê°•ì œ ì‚¬ìš© (Apple Silicon í˜¸í™˜ì„±)
                    'trust_remote_code': True,
                }

                # ìºì‹œ ë””ë ‰í† ë¦¬ê°€ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
                if "SENTENCE_TRANSFORMERS_HOME" in os.environ:
                    logger.info(f"Using cache directory: {os.environ['SENTENCE_TRANSFORMERS_HOME']}")

                _model_instance = SentenceTransformer(
                    model_to_load,
                    **model_kwargs
                )

                # ëª¨ë¸ ì´ë¦„ ì•ˆì „í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
                actual_model_name = get_model_name(_model_instance)
                logger.info(f"Successfully loaded model: {model_to_load} (actual: {actual_model_name})")
                logger.info(f"Model max sequence length: {_model_instance.max_seq_length}")

                # ëª¨ë¸ í…ŒìŠ¤íŠ¸
                test_text = "í…ŒìŠ¤íŠ¸ í…ìŠ¤íŠ¸"
                test_embedding = _model_instance.encode([test_text], convert_to_tensor=False)
                logger.info(f"Model test successful. Embedding shape: {test_embedding.shape}")

                break

            except Exception as e:
                logger.warning(f"Failed to load model {model_to_load}: {e}")
                if idx == len(models_to_try) - 1:
                    raise RuntimeError(f"All models failed to load. Last error: {e}")
                continue

        return _model_instance

    except Exception as e:
        logger.error(f"Model loading failed: {e}")
        raise RuntimeError(f"Embedding model loading failed: {e}")

    finally:
        _model_lock = False


@lru_cache(maxsize=128)
def _cached_embed_single(text: str, prefix: str = "query") -> np.ndarray:
    """
    ë‹¨ì¼ í…ìŠ¤íŠ¸ ì„ë² ë”© (ìºì‹œë¨)

    Args:
        text: ì„ë² ë”©í•  í…ìŠ¤íŠ¸
        prefix: ì„ë² ë”© prefix (query, passage)

    Returns:
        ì •ê·œí™”ëœ ì„ë² ë”© ë²¡í„°
    """
    try:
        model = get_model()

        # E5 ëª¨ë¸ì˜ ê²½ìš° prefix ì¶”ê°€ (ëª¨ë¸ ì´ë¦„ ì•ˆì „í•˜ê²Œ í™•ì¸)
        model_name = get_model_name(model).lower()
        if "e5" in model_name:
            prefixed_text = f"{prefix}: {text}"
        else:
            prefixed_text = text

        embedding = model.encode(
            [prefixed_text],
            batch_size=1,
            convert_to_tensor=False,
            normalize_embeddings=True,
            show_progress_bar=False
        )

        return embedding[0]

    except Exception as e:
        logger.error(f"Single embedding failed: {e}")
        raise


def embed_texts(texts: List[str], prefix: str = "query", batch_size: int = 32) -> np.ndarray:
    """
    í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ ë³€í™˜

    Args:
        texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
        prefix: ì„ë² ë”© prefix (E5 ëª¨ë¸ìš©)
        batch_size: ë°°ì¹˜ í¬ê¸°

    Returns:
        ì •ê·œí™”ëœ ì„ë² ë”© ë²¡í„° ë°°ì—´ (shape: [len(texts), embedding_dim])

    Raises:
        RuntimeError: ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ì‹œ
    """
    if not texts:
        return np.array([])

    try:
        # ë‹¨ì¼ í…ìŠ¤íŠ¸ì¸ ê²½ìš° ìºì‹œëœ í•¨ìˆ˜ ì‚¬ìš©
        if len(texts) == 1:
            return np.array([_cached_embed_single(texts[0], prefix)])

        # ë³µìˆ˜ í…ìŠ¤íŠ¸ì¸ ê²½ìš°
        model = get_model()

        logger.info(f"Generating embeddings for {len(texts)} texts")

        # E5 ëª¨ë¸ì˜ ê²½ìš° prefix ì¶”ê°€ (ëª¨ë¸ ì´ë¦„ ì•ˆì „í•˜ê²Œ í™•ì¸)
        model_name = get_model_name(model).lower()
        if "e5" in model_name:
            prefixed_texts = [f"{prefix}: {text}" for text in texts]
        else:
            prefixed_texts = texts

        # ë°°ì¹˜ ì²˜ë¦¬
        embeddings = model.encode(
            prefixed_texts,
            batch_size=batch_size,
            convert_to_tensor=False,
            normalize_embeddings=True,
            show_progress_bar=len(texts) > 10,  # 10ê°œ ì´ìƒì¼ ë•Œë§Œ ì§„í–‰ë¥  í‘œì‹œ
            device='cpu'  # CPU ê°•ì œ ì‚¬ìš©
        )

        logger.info(f"Successfully generated embeddings: {embeddings.shape}")
        return embeddings

    except Exception as e:
        logger.error(f"Batch embedding failed: {e}")
        raise RuntimeError(f"Failed to generate embeddings: {e}")


def get_embedding_dimension(model_name: str = DEFAULT_MODEL) -> int:
    """
    ì„ë² ë”© ëª¨ë¸ì˜ ì°¨ì› ìˆ˜ ë°˜í™˜

    Args:
        model_name: ëª¨ë¸ëª…

    Returns:
        ì„ë² ë”© ì°¨ì› ìˆ˜
    """
    try:
        model = get_model(model_name)
        # í…ŒìŠ¤íŠ¸ ì„ë² ë”©ìœ¼ë¡œ ì°¨ì› í™•ì¸
        test_embedding = model.encode(["test"], convert_to_tensor=False)
        return test_embedding.shape[1]
    except Exception as e:
        logger.error(f"Failed to get embedding dimension: {e}")
        # ê¸°ë³¸ê°’ ë°˜í™˜
        return 1024 if "large" in model_name else 384


def clear_model_cache():
    """ëª¨ë¸ ìºì‹œ ì •ë¦¬"""
    global _model_instance
    _model_instance = None
    _cached_embed_single.cache_clear()
    logger.info("Model cache cleared")


def get_model_info() -> dict:
    """í˜„ì¬ ë¡œë“œëœ ëª¨ë¸ ì •ë³´ ë°˜í™˜"""
    try:
        model = get_model()
        model_name = get_model_name(model)
        return {
            "model_name": model_name,
            "max_seq_length": getattr(model, 'max_seq_length', 'unknown'),
            "device": str(getattr(model, 'device', 'unknown')),
            "cache_dir": os.environ.get("SENTENCE_TRANSFORMERS_HOME", "default"),
            "embedding_dimension": get_embedding_dimension()
        }
    except Exception as e:
        return {"error": str(e)}


# ëª¨ë“ˆ ì´ˆê¸°í™” ì‹œ í™˜ê²½ í™•ì¸
def _check_environment():
    """í™˜ê²½ ì„¤ì • í™•ì¸"""
    logger.info("Checking embedding environment...")

    # ìºì‹œ ë””ë ‰í† ë¦¬ í™•ì¸
    cache_dirs = {
        "transformers": os.environ.get("TRANSFORMERS_CACHE"),
        "huggingface": os.environ.get("HF_HOME"),
        "torch": os.environ.get("TORCH_HOME"),
        "sentence_transformers": os.environ.get("SENTENCE_TRANSFORMERS_HOME")
    }

    for name, path in cache_dirs.items():
        if path and Path(path).exists():
            logger.info(f"âœ… {name} cache: {path}")
        else:
            logger.warning(f"âš ï¸  {name} cache not set or missing: {path}")


# ëª¨ë“ˆ ë¡œë“œ ì‹œ í™˜ê²½ ì²´í¬
if __name__ != "__main__":
    _check_environment()


# í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("ğŸ§ª Embedding ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")

    try:
        # 1. í™˜ê²½ í™•ì¸
        print("\n1. í™˜ê²½ ì„¤ì • í™•ì¸:")
        _check_environment()

        # 2. ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸
        print("\n2. ëª¨ë¸ ë¡œë“œ í…ŒìŠ¤íŠ¸:")
        model_info = get_model_info()
        for key, value in model_info.items():
            print(f"   {key}: {value}")

        # 3. ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸
        print("\n3. ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸:")
        test_texts = [
            "ì•ˆë…•í•˜ì„¸ìš”, ì„¸ìƒ!",
            "Hello, world!",
            "ë¬¸ì„œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ ì‹œìŠ¤í…œ"
        ]

        embeddings = embed_texts(test_texts)
        print(f"   í…ìŠ¤íŠ¸ ìˆ˜: {len(test_texts)}")
        print(f"   ì„ë² ë”© í˜•íƒœ: {embeddings.shape}")
        print(f"   ì²« ë²ˆì§¸ ì„ë² ë”© (ì²˜ìŒ 5ê°œ): {embeddings[0][:5]}")

        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()