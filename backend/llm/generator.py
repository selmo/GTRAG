"""
LLM ìƒì„± ëª¨ë“ˆ: Ollamaë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„±
"""
import os
import requests
import logging
from typing import List, Dict, Optional

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

OLLAMA_HOST = os.getenv("OLLAMA_HOST", "http://172.16.15.112:11434")
OLLAMA_MODEL = os.getenv("OLLAMA_MODEL", "gemma3:27b")


def generate_answer(query: str, contexts: List[str], model: str = None, system_prompt: str = None) -> str:
    """
    ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„±

    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        contexts: ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œë“¤
        model: ì‚¬ìš©í•  ëª¨ë¸ (ê¸°ë³¸ê°’: í™˜ê²½ë³€ìˆ˜ì˜ OLLAMA_MODEL)

    Returns:
        ìƒì„±ëœ ë‹µë³€
    """
    if not model:
        model = OLLAMA_MODEL

    # ğŸ” ë””ë²„ê·¸ ë¡œê·¸ 1: ì…ë ¥ ë°ì´í„° í™•ì¸
    logger.info(f"=== OLLAMA GENERATION DEBUG ===")
    logger.info(f"Query: '{query}'")
    logger.info(f"Model: {model}")
    logger.info(f"Host: {OLLAMA_HOST}")
    logger.info(f"Context count: {len(contexts)}")

    # ì»¨í…ìŠ¤íŠ¸ ìƒì„¸ ë¡œê·¸
    for i, ctx in enumerate(contexts):
        preview = ctx[:100] + "..." if len(ctx) > 100 else ctx
        logger.info(f"  Context {i + 1}: {len(ctx)} chars - '{preview}'")

    if not contexts:
        logger.warning("No contexts provided!")
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ì°¸ê³ í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

    # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    context_text = "\n---\n".join(contexts)

    # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (Ollama í† í° ì œí•œ ê³ ë ¤)
    max_context_length = 3000  # ì•½ 4000í† í° ì œí•œ
    if len(context_text) > max_context_length:
        context_text = context_text[:max_context_length] + "\n...(ë‚´ìš© ìƒëµ)"
        logger.info(f"Context truncated to {max_context_length} characters")

    # ğŸ”§ ê°œì„ ëœ í”„ë¡¬í”„íŠ¸ (í•œêµ­ì–´ íŠ¹í™”)
    if system_prompt:
        prompt = f"""{system_prompt}

ì°¸ê³  ë¬¸ì„œ:
{context_text}

ì§ˆë¬¸: {query}

ë‹µë³€: """
    else:
        prompt = f"""ë‹¹ì‹ ì€ í•œêµ­ì–´ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ë¬¸ì„œì˜ ë‚´ìš©ë§Œì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

ì°¸ê³  ë¬¸ì„œ:
{context_text}

ì§ˆë¬¸: {query}

ë‹µë³€ì„ ì‘ì„±í•  ë•Œ ë‹¤ìŒ ê·œì¹™ì„ ë”°ë¼ì£¼ì„¸ìš”:
1. ë¬¸ì„œì— ëª…ì‹œëœ ë‚´ìš©ë§Œ ì‚¬ìš©í•˜ì„¸ìš”
2. ì¶”ì¸¡í•˜ê±°ë‚˜ ì™¸ë¶€ ì§€ì‹ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”
3. í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”
4. êµ¬ì²´ì ì¸ ì •ë³´ê°€ ìˆë‹¤ë©´ ì •í™•íˆ ì¸ìš©í•´ì£¼ì„¸ìš”

ë‹µë³€:"""

    # ğŸ” ë””ë²„ê·¸ ë¡œê·¸ 2: í”„ë¡¬í”„íŠ¸ í™•ì¸
    logger.info(f"Prompt: {prompt}")
    logger.info(f"Prompt length: {len(prompt)} characters")
    logger.info(f"Prompt preview: {prompt[:200]}...")

    # Ollama API í˜¸ì¶œ
    try:
        # ğŸ” ë””ë²„ê·¸ ë¡œê·¸ 3: API ìš”ì²­ ë°ì´í„°
        request_data = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.3,
                "top_p": 0.9,
                "num_predict": 500,  # max_tokens ëŒ€ì‹  num_predict ì‚¬ìš©
                "stop": ["\n\nì§ˆë¬¸:", "\n\nì°¸ê³  ë¬¸ì„œ:"]  # ì¤‘ì§€ í† í° ì¶”ê°€
            }
        }

        logger.info(f"Ollama request: {OLLAMA_HOST}/api/generate")
        logger.info(f"Request options: {request_data['options']}")

        response = requests.post(
            f"{OLLAMA_HOST}/api/generate",
            json=request_data,
            timeout=300  # íƒ€ì„ì•„ì›ƒ ëŠ˜ë¦¼
        )

        # ğŸ” ë””ë²„ê·¸ ë¡œê·¸ 4: API ì‘ë‹µ ìƒì„¸
        logger.info(f"Ollama response status: {response.status_code}")
        logger.info(f"Ollama response headers: {dict(response.headers)}")

        if response.status_code == 200:
            response_data = response.json()
            logger.info(f"Ollama response keys: {list(response_data.keys())}")

            generated_text = response_data.get("response", "")
            logger.info(f"Generated text length: {len(generated_text)}")
            logger.info(f"Generated text preview: {generated_text[:200]}...")

            # ë¹ˆ ì‘ë‹µ í™•ì¸
            if not generated_text.strip():
                logger.warning("Ollama returned empty response!")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. AIê°€ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ì§ˆë¬¸ì„ ì‹œë„í•´ë³´ì„¸ìš”."

            # ì‘ë‹µ í›„ì²˜ë¦¬
            cleaned_response = generated_text.strip()

            # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°
            if "ë‹µë³€:" in cleaned_response:
                cleaned_response = cleaned_response.split("ë‹µë³€:")[-1].strip()

            logger.info(f"Final answer: {cleaned_response[:100]}...")
            return cleaned_response

        else:
            # ğŸ” ë””ë²„ê·¸ ë¡œê·¸ 5: ì˜¤ë¥˜ ì‘ë‹µ ìƒì„¸
            error_text = response.text
            logger.error(f"Ollama error response: {error_text}")
            return f"ì˜¤ë¥˜: Ollama ì„œë²„ ì‘ë‹µ ì‹¤íŒ¨ (ìƒíƒœì½”ë“œ: {response.status_code}) - {error_text[:200]}"

    except requests.exceptions.ConnectionError as e:
        logger.error(f"Ollama connection error: {e}")
        return f"ì˜¤ë¥˜: Ollama ì„œë²„ ({OLLAMA_HOST})ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. - {str(e)}"
    except requests.exceptions.Timeout as e:
        logger.error(f"Ollama timeout error: {e}")
        return f"ì˜¤ë¥˜: Ollama ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ - {str(e)}"
    except Exception as e:
        logger.error(f"Ollama unexpected error: {e}")
        return f"ì˜¤ë¥˜: ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ - {str(e)}"


def check_ollama_connection() -> Dict[str, any]:
    """Ollama ì„œë²„ ì—°ê²° ìƒíƒœ í™•ì¸ (ë””ë²„ê·¸ ê°•í™”)"""
    try:
        logger.info(f"Checking Ollama connection: {OLLAMA_HOST}")

        response = requests.get(f"{OLLAMA_HOST}/api/tags", timeout=5)
        logger.info(f"Ollama tags response: {response.status_code}")

        if response.status_code == 200:
            data = response.json()
            models = data.get("models", [])
            model_names = [m.get("name", "unknown") for m in models]

            logger.info(f"Available models: {model_names}")

            # ê¸°ë³¸ ëª¨ë¸ í™•ì¸
            default_model_available = OLLAMA_MODEL in model_names
            logger.info(f"Default model '{OLLAMA_MODEL}' available: {default_model_available}")

            return {
                "status": "connected",
                "host": OLLAMA_HOST,
                "models": model_names,
                "total_models": len(model_names),
                "default_model_available": default_model_available
            }
    except Exception as e:
        logger.error(f"Ollama connection check failed: {e}")

    return {
        "status": "disconnected",
        "host": OLLAMA_HOST,
        "error": "Cannot connect to Ollama server"
    }


# ğŸ” ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ ì¶”ê°€
def test_ollama_simple():
    """Ollama ê°„ë‹¨ í…ŒìŠ¤íŠ¸"""
    test_query = "ì•ˆë…•í•˜ì„¸ìš”"
    test_contexts = ["í…ŒìŠ¤íŠ¸ ë¬¸ì„œì…ë‹ˆë‹¤."]

    logger.info("=== OLLAMA SIMPLE TEST ===")
    result = generate_answer(test_query, test_contexts)
    logger.info(f"Test result: {result}")
    return result
