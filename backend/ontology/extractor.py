"""
Ontology ì¶”ì¶œ ì—”ì§„ - ì €ë¹„ìš© ê³ íš¨ìœ¨ ë²„ì „
KeyBERT + spaCy + ê¸°ì¡´ ì„ë² ë”© ëª¨ë¸ í™œìš©
"""
import re
import logging
from typing import Dict, List, Tuple, Optional, Any
from collections import Counter, defaultdict
from dataclasses import dataclass, asdict
from datetime import datetime
import numpy as np

# ê¸°ì¡´ ì‹œìŠ¤í…œ ì„í¬íŠ¸
from backend.embedding.embedder import get_model, embed_texts

logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°ì´í„° ëª¨ë¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class KeywordInfo:
    term: str
    score: float
    frequency: int
    category: str  # 'technical', ...
    positions: List[int]
    description: str = ""  # ğŸ” ìƒˆ í•„ë“œ ì¶”ê°€


@dataclass
class EntityInfo:
    """ê°œì²´ ì •ë³´"""
    text: str
    label: str  # PERSON, ORG, GPE, DATE, MONEY, etc.
    start: int
    end: int
    confidence: float


@dataclass
class DocumentMetadata:
    """ë¬¸ì„œ ë©”íƒ€ë°ì´í„°"""
    language: str
    document_type: str
    estimated_domain: str
    key_entities: List[EntityInfo]
    text_statistics: Dict[str, Any]
    structure_info: Dict[str, Any]


@dataclass
class ContextInfo:
    """ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸"""
    main_topics: List[str]
    semantic_clusters: List[Dict[str, Any]]
    related_concepts: List[str]
    domain_indicators: List[str]


@dataclass
class OntologyResult:
    """ì˜¨í†¨ë¡œì§€ ì¶”ì¶œ ê²°ê³¼"""
    doc_id: str
    source: str
    keywords: List[KeywordInfo]
    metadata: DocumentMetadata
    context: ContextInfo
    extracted_at: datetime
    processing_stats: Dict[str, float]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í‚¤ì›Œë“œ ì¶”ì¶œê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class KeywordExtractor:
    """KeyBERT ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""

    def __init__(self):
        self.model = None
        self._setup_keybert()

    def _setup_keybert(self):
        """KeyBERT ì„¤ì •"""
        try:
            from keybert import KeyBERT
            # ê¸°ì¡´ ì„ë² ë”© ëª¨ë¸ ì¬ì‚¬ìš©
            embedding_model = get_model()
            self.model = KeyBERT(model=embedding_model)
            logger.info("KeyBERT initialized with existing embedding model")
        except ImportError:
            logger.warning("KeyBERT not available, falling back to TF-IDF")
            self.model = None

    def extract_keywords(self, text: str, existing_keywords: List[str] = None, top_k: int = 20) -> List[KeywordInfo]:
        existing_keywords = existing_keywords or []

        if not text or len(text.strip()) < 10:
            return []

        keywords = []
        if self.model:
            keywords.extend(self._extract_with_keybert(text, top_k))
        else:
            keywords.extend(self._extract_with_tfidf(text, top_k))

        keywords.extend(self._extract_statistical(text, top_k // 2))

        return self._deduplicate_keywords(keywords, top_k)


    def _extract_with_keybert(self, text: str, top_k: int) -> List[KeywordInfo]:
        """KeyBERTë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        try:
            # KeyBERT ìµœì‹  API í˜¸í™˜ì„± í™•ì¸ ë° ë‹¤ì–‘í•œ n-gram ì¡°í•© ì¶”ì¶œ
            kwargs_base = {
                'docs': text,
                'keyphrase_ngram_range': (1, 1),
                'stop_words': 'english'
            }

            # KeyBERT ë²„ì „ë³„ íŒŒë¼ë¯¸í„° í˜¸í™˜ì„± ì²˜ë¦¬
            try:
                # ìµœì‹  KeyBERT: top_k íŒŒë¼ë¯¸í„° ì‚¬ìš©
                keywords_1gram = self.model.extract_keywords(**kwargs_base, top_k=top_k//2)
            except TypeError:
                try:
                    # êµ¬ë²„ì „ KeyBERT: top_k ëŒ€ì‹  ë‹¤ë¥¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©
                    keywords_1gram = self.model.extract_keywords(**kwargs_base)[:top_k//2]
                except Exception:
                    # ê¸°ë³¸ ì¶”ì¶œ
                    keywords_1gram = self.model.extract_keywords(text)[:top_k//2]

            try:
                kwargs_base['keyphrase_ngram_range'] = (2, 2)
                keywords_2gram = self.model.extract_keywords(**kwargs_base, top_k=top_k//3)
            except TypeError:
                try:
                    keywords_2gram = self.model.extract_keywords(**kwargs_base)[:top_k//3]
                except Exception:
                    keywords_2gram = self.model.extract_keywords(text, keyphrase_ngram_range=(2, 2))[:top_k//3]

            try:
                kwargs_base['keyphrase_ngram_range'] = (3, 3)
                keywords_3gram = self.model.extract_keywords(**kwargs_base, top_k=top_k//6)
            except TypeError:
                try:
                    keywords_3gram = self.model.extract_keywords(**kwargs_base)[:top_k//6]
                except Exception:
                    keywords_3gram = self.model.extract_keywords(text, keyphrase_ngram_range=(3, 3))[:top_k//6]

            results = []
            for keyword, score in keywords_1gram + keywords_2gram + keywords_3gram:
                # í‚¤ì›Œë“œ ìœ„ì¹˜ ì°¾ê¸°
                positions = [m.start() for m in re.finditer(re.escape(keyword.lower()), text.lower())]
                frequency = len(positions)

                if frequency > 0:
                    results.append(KeywordInfo(
                        term=keyword,
                        score=float(score),
                        frequency=frequency,
                        category=self._classify_keyword(keyword),
                        positions=positions[:5],
                        description=self._generate_description(keyword, text)
                    ))

            return results

        except Exception as e:
            logger.warning(f"KeyBERT extraction failed: {e}")
            return []

    def _extract_with_tfidf(self, text: str, top_k: int) -> List[KeywordInfo]:
        """TF-IDF í´ë°± ì¶”ì¶œ"""
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS

            # í•œêµ­ì–´ ë¶ˆìš©ì–´ ì¶”ê°€
            korean_stop_words = set(['ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ë˜ëŠ”', 'ê·¸ë¦¬ê³ ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ¬ë‚˜'])
            stop_words = list(ENGLISH_STOP_WORDS) + list(korean_stop_words)

            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            sentences = re.split(r'[.!?]\s+', text)
            if len(sentences) < 2:
                sentences = [text]

            vectorizer = TfidfVectorizer(
                max_features=top_k * 2,
                stop_words=stop_words,
                ngram_range=(1, 3),
                min_df=1,
                max_df=0.95
            )

            tfidf_matrix = vectorizer.fit_transform(sentences)
            feature_names = vectorizer.get_feature_names_out()

            # í‰ê·  TF-IDF ìŠ¤ì½”ì–´ ê³„ì‚°
            mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)

            results = []
            for idx, score in enumerate(mean_scores):
                if score > 0.1:  # ì„ê³„ê°’
                    keyword = feature_names[idx]
                    positions = [m.start() for m in re.finditer(re.escape(keyword.lower()), text.lower())]
                    frequency = len(positions)

                    if frequency > 0:
                        results.append(KeywordInfo(
                            term=keyword,
                            score=float(score),
                            frequency=frequency,
                            category=self._classify_keyword(keyword),
                            positions=positions[:5],
                            description=self._generate_description(keyword, text)
                        ))

            return sorted(results, key=lambda x: x.score, reverse=True)[:top_k]

        except Exception as e:
            logger.warning(f"TF-IDF extraction failed: {e}")
            return []

    def _extract_statistical(self, text: str, top_k: int) -> List[KeywordInfo]:
        """í†µê³„ ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜
        words = re.findall(r'\b[ê°€-í£a-zA-Z]{2,}\b', text.lower())
        word_freq = Counter(words)

        results = []
        for word, freq in word_freq.most_common(top_k):
            if freq >= 2:  # ìµœì†Œ 2ë²ˆ ì´ìƒ ë“±ì¥
                positions = [m.start() for m in re.finditer(re.escape(word), text.lower())]
                results.append(KeywordInfo(
                    term=word,
                    score=freq / len(words),  # ì •ê·œí™”ëœ ë¹ˆë„
                    frequency=freq,
                    category=self._classify_keyword(word),
                    positions=positions[:5],
                    description=self._generate_description(word, text)
                ))

        return results

    def _classify_keyword(self, keyword: str) -> str:
        """í‚¤ì›Œë“œ ë¶„ë¥˜"""
        keyword_lower = keyword.lower()

        # ê¸°ìˆ  ìš©ì–´ íŒ¨í„´
        if any(pattern in keyword_lower for pattern in ['api', 'system', 'data', 'model', 'algorithm', 'tech']):
            return 'technical'

        # ì¡°ì§ íŒ¨í„´
        if any(pattern in keyword_lower for pattern in ['company', 'corp', 'inc', 'íšŒì‚¬', 'ê¸°ì—…', 'ë‹¨ì²´']):
            return 'organization'

        # ìœ„ì¹˜ íŒ¨í„´
        if any(pattern in keyword_lower for pattern in ['city', 'country', 'ì‹œ', 'êµ¬', 'ë™', 'ë¡œ', 'êµ­']):
            return 'location'

        # ì‚¬ëŒ ì´ë¦„ íŒ¨í„´ (í•œêµ­ì–´)
        if re.match(r'^[ê°€-í£]{2,4}$', keyword) and len(keyword) <= 4:
            return 'person'

        return 'general'

    def _deduplicate_keywords(self, keywords: List[KeywordInfo], top_k: int) -> List[KeywordInfo]:
        """í‚¤ì›Œë“œ ì¤‘ë³µ ì œê±° ë° ì •ë ¬"""
        # ìœ ì‚¬í•œ í‚¤ì›Œë“œ í†µí•©
        seen = {}
        for kw in keywords:
            key = kw.term.lower().strip()
            if key not in seen or seen[key].score < kw.score:
                seen[key] = kw

        # ìŠ¤ì½”ì–´ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ kê°œ ë°˜í™˜
        return sorted(seen.values(), key=lambda x: x.score, reverse=True)[:top_k]

    def _generate_description(self, keyword: str, context: str) -> str:
        """í‚¤ì›Œë“œ ì„¤ëª… ìƒì„± - ë‹¨ìˆœ ê·œì¹™ ê¸°ë°˜"""
        # ë¬¸ì¥ì—ì„œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì²« ë¬¸ì¥ì„ ì°¾ì•„ ì„¤ëª…ìœ¼ë¡œ ì‚¬ìš©
        import re
        sentences = re.split(r'[.!?]\s+', context)
        for sent in sentences:
            if keyword.lower() in sent.lower():
                return sent.strip()[:200]  # ë„ˆë¬´ ê¸¸ë©´ ìë¦„
        return f"'{keyword}'ëŠ” ë¬¸ì„œì—ì„œ ì¤‘ìš”í•œ ê°œë…ì…ë‹ˆë‹¤."


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class MetadataExtractor:
    """spaCy ê¸°ë°˜ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""

    def __init__(self):
        self.nlp = None
        self._setup_spacy()

    def _setup_spacy(self):
        """spaCy ì„¤ì •"""
        try:
            import spacy

            # ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì‹œë„
            models_to_try = ['ko_core_news_sm', 'en_core_web_sm', 'xx_core_web_sm']

            for model_name in models_to_try:
                try:
                    self.nlp = spacy.load(model_name)
                    logger.info(f"Loaded spaCy model: {model_name}")
                    break
                except OSError:
                    continue

            if not self.nlp:
                logger.warning("No spaCy model available, using basic extraction")

        except ImportError:
            logger.warning("spaCy not available")

    def extract_metadata(self, text: str, source: str) -> DocumentMetadata:
        """ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
        # ê¸°ë³¸ í†µê³„
        text_stats = self._analyze_text_statistics(text)

        # ì–¸ì–´ ê°ì§€
        language = self._detect_language(text)

        # ë¬¸ì„œ ìœ í˜• ì¶”ì •
        doc_type = self._estimate_document_type(text, source)

        # ë„ë©”ì¸ ì¶”ì •
        domain = self._estimate_domain(text)

        # ê°œì²´ëª… ì¸ì‹
        entities = self._extract_entities(text) if self.nlp else []

        # êµ¬ì¡° ë¶„ì„
        structure = self._analyze_structure(text)

        return DocumentMetadata(
            language=language,
            document_type=doc_type,
            estimated_domain=domain,
            key_entities=entities,
            text_statistics=text_stats,
            structure_info=structure
        )

    def _analyze_text_statistics(self, text: str) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„"""
        lines = text.split('\n')
        words = re.findall(r'\b\w+\b', text)
        sentences = re.split(r'[.!?]+', text)

        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        numbers = len(re.findall(r'\d+', text))

        return {
            'total_length': len(text),
            'lines': len([line for line in lines if line.strip()]),
            'words': len(words),
            'sentences': len([s for s in sentences if s.strip()]),
            'korean_chars': korean_chars,
            'english_chars': english_chars,
            'numbers': numbers,
            'avg_word_length': sum(len(w) for w in words) / len(words) if words else 0,
            'avg_sentence_length': len(words) / len(sentences) if sentences else 0
        }

    def _detect_language(self, text: str) -> str:
        """ì–¸ì–´ ê°ì§€"""
        korean_chars = len(re.findall(r'[ê°€-í£]', text))
        english_chars = len(re.findall(r'[a-zA-Z]', text))
        total_chars = korean_chars + english_chars

        if total_chars == 0:
            return 'unknown'

        korean_ratio = korean_chars / total_chars

        if korean_ratio > 0.3:
            return 'korean'
        elif korean_ratio < 0.1:
            return 'english'
        else:
            return 'mixed'

    def _estimate_document_type(self, text: str, source: str) -> str:
        """ë¬¸ì„œ ìœ í˜• ì¶”ì •"""
        source_lower = source.lower()
        text_lower = text.lower()

        # íŒŒì¼ëª… ê¸°ë°˜
        if any(ext in source_lower for ext in ['.pdf', '.doc']):
            if any(word in source_lower for word in ['contract', 'ê³„ì•½', 'agreement']):
                return 'contract'
            elif any(word in source_lower for word in ['report', 'ë³´ê³ ì„œ', 'analysis']):
                return 'report'
            elif any(word in source_lower for word in ['manual', 'ë§¤ë‰´ì–¼', 'guide']):
                return 'manual'

        # ë‚´ìš© ê¸°ë°˜
        if any(pattern in text_lower for pattern in ['article', 'ì¡°', 'í•­', 'ë¶€ì¹™']):
            return 'legal'
        elif any(pattern in text_lower for pattern in ['abstract', 'ìš”ì•½', 'conclusion']):
            return 'academic'
        elif any(pattern in text_lower for pattern in ['procedure', 'ì ˆì°¨', 'step']):
            return 'procedure'

        return 'general'

    def _estimate_domain(self, text: str) -> str:
        """ë„ë©”ì¸ ì¶”ì •"""
        text_lower = text.lower()

        domain_keywords = {
            'technology': ['api', 'system', 'software', 'algorithm', 'data', 'ai', 'ml', 'ì¸ê³µì§€ëŠ¥'],
            'finance': ['money', 'investment', 'financial', 'budget', 'ì˜ˆì‚°', 'íˆ¬ì', 'ê¸ˆìœµ'],
            'legal': ['law', 'regulation', 'legal', 'court', 'ë²•ë¥ ', 'ê·œì •', 'ë²•ì›'],
            'medical': ['health', 'medical', 'patient', 'treatment', 'ì˜ë£Œ', 'í™˜ì', 'ì¹˜ë£Œ'],
            'business': ['business', 'management', 'strategy', 'market', 'ë¹„ì¦ˆë‹ˆìŠ¤', 'ê²½ì˜', 'ì „ëµ'],
            'academic': ['research', 'study', 'analysis', 'theory', 'ì—°êµ¬', 'ë¶„ì„', 'ì´ë¡ ']
        }

        domain_scores = {}
        for domain, keywords in domain_keywords.items():
            score = sum(text_lower.count(keyword) for keyword in keywords)
            if score > 0:
                domain_scores[domain] = score

        if domain_scores:
            return max(domain_scores, key=domain_scores.get)

        return 'general'

    def _extract_entities(self, text: str) -> List[EntityInfo]:
        """ê°œì²´ëª… ì¶”ì¶œ"""
        if not self.nlp:
            return []

        try:
            doc = self.nlp(text)
            entities = []

            for ent in doc.ents:
                entities.append(EntityInfo(
                    text=ent.text,
                    label=ent.label_,
                    start=ent.start_char,
                    end=ent.end_char,
                    confidence=1.0  # spaCyëŠ” ì‹ ë¢°ë„ë¥¼ ì œê³µí•˜ì§€ ì•ŠìŒ
                ))

            return entities

        except Exception as e:
            logger.warning(f"Entity extraction failed: {e}")
            return []

    def _analyze_structure(self, text: str) -> Dict[str, Any]:
        """ë¬¸ì„œ êµ¬ì¡° ë¶„ì„"""
        lines = text.split('\n')

        # ì œëª© ë¼ì¸ ê°ì§€ (ì§§ê³  ëŒ€ë¬¸ìê°€ ë§ì€ ë¼ì¸)
        potential_headers = []
        for i, line in enumerate(lines):
            line = line.strip()
            if line and len(line) < 100:
                upper_ratio = sum(1 for c in line if c.isupper()) / len(line)
                if upper_ratio > 0.3:
                    potential_headers.append((i, line))

        # ëª©ë¡ ê°ì§€
        list_items = []
        for i, line in enumerate(lines):
            line = line.strip()
            if re.match(r'^\d+\.|\*|\-|â€¢', line):
                list_items.append((i, line))

        return {
            'total_lines': len(lines),
            'empty_lines': len([line for line in lines if not line.strip()]),
            'potential_headers': len(potential_headers),
            'list_items': len(list_items),
            'has_numbered_sections': bool(re.search(r'\d+\.\d+', text)),
            'has_bullet_points': bool(re.search(r'[â€¢\*\-]\s+', text))
        }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ContextExtractor:
    """ì„ë² ë”© ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""

    def __init__(self):
        self.embedding_model = get_model()

    def extract_context(self, text: str, chunks: List[str] = None) -> ContextInfo:
        """ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        if not chunks:
            chunks = self._split_into_chunks(text)

        # ì£¼ìš” í† í”½ ì¶”ì¶œ
        main_topics = self._extract_topics(chunks)

        # ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°ë§
        semantic_clusters = self._cluster_chunks(chunks)

        # ê´€ë ¨ ê°œë… ì¶”ì¶œ
        related_concepts = self._extract_related_concepts(text)

        # ë„ë©”ì¸ ì§€ì‹œì ì¶”ì¶œ
        domain_indicators = self._extract_domain_indicators(text)

        return ContextInfo(
            main_topics=main_topics,
            semantic_clusters=semantic_clusters,
            related_concepts=related_concepts,
            domain_indicators=domain_indicators
        )

    def _split_into_chunks(self, text: str, chunk_size: int = 300) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• """
        words = text.split()
        chunks = []

        for i in range(0, len(words), chunk_size):
            chunk = ' '.join(words[i:i + chunk_size])
            if len(chunk.strip()) > 50:  # ë„ˆë¬´ ì§§ì€ ì²­í¬ ì œì™¸
                chunks.append(chunk)

        return chunks

    def _extract_topics(self, chunks: List[str], top_k: int = 5) -> List[str]:
        """ì£¼ìš” í† í”½ ì¶”ì¶œ"""
        if len(chunks) < 2:
            return []

        try:
            from sklearn.cluster import KMeans
            from sklearn.feature_extraction.text import TfidfVectorizer

            # TF-IDF ë²¡í„°í™”
            vectorizer = TfidfVectorizer(max_features=100, stop_words='english')
            tfidf_matrix = vectorizer.fit_transform(chunks)

            # í´ëŸ¬ìŠ¤í„°ë§
            n_clusters = min(top_k, len(chunks))
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(tfidf_matrix)

            # ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ìš©ì–´ ì¶”ì¶œ
            feature_names = vectorizer.get_feature_names_out()
            topics = []

            for cluster_id in range(n_clusters):
                # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì˜ ìƒìœ„ ìš©ì–´ë“¤
                center = kmeans.cluster_centers_[cluster_id]
                top_indices = center.argsort()[-3:][::-1]  # ìƒìœ„ 3ê°œ
                topic_terms = [feature_names[i] for i in top_indices]
                topics.append(' '.join(topic_terms))

            return topics

        except Exception as e:
            logger.warning(f"Topic extraction failed: {e}")
            return []

    def _cluster_chunks(self, chunks: List[str]) -> List[Dict[str, Any]]:
        """ì²­í¬ë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„°ë§"""
        if len(chunks) < 2:
            return []

        try:
            # ì„ë² ë”© ìƒì„±
            embeddings = embed_texts(chunks, prefix="passage")

            from sklearn.cluster import KMeans
            from sklearn.metrics.pairwise import cosine_similarity

            # í´ëŸ¬ìŠ¤í„°ë§
            n_clusters = min(5, len(chunks))
            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(embeddings)

            # í´ëŸ¬ìŠ¤í„° ì •ë³´ êµ¬ì„±
            clusters = []
            for cluster_id in range(n_clusters):
                cluster_chunks = [chunks[i] for i, label in enumerate(cluster_labels) if label == cluster_id]
                cluster_embeddings = embeddings[cluster_labels == cluster_id]

                # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ê³¼ì˜ ìœ ì‚¬ë„ ê³„ì‚°
                center = kmeans.cluster_centers_[cluster_id].reshape(1, -1)
                similarities = cosine_similarity(cluster_embeddings, center).flatten()

                # ëŒ€í‘œ ì²­í¬ ì„ íƒ
                representative_idx = similarities.argmax()
                representative_chunk = cluster_chunks[representative_idx]

                clusters.append({
                    'cluster_id': cluster_id,
                    'size': len(cluster_chunks),
                    'representative_chunk': representative_chunk[:200] + '...' if len(representative_chunk) > 200 else representative_chunk,
                    'avg_similarity': float(similarities.mean()),
                    'chunk_indices': [i for i, label in enumerate(cluster_labels) if label == cluster_id]
                })

            return clusters

        except Exception as e:
            logger.warning(f"Chunk clustering failed: {e}")
            return []

    def _extract_related_concepts(self, text: str) -> List[str]:
        """ê´€ë ¨ ê°œë… ì¶”ì¶œ"""
        # ëª…ì‚¬êµ¬ íŒ¨í„´ ë§¤ì¹­
        noun_phrases = re.findall(r'[ê°€-í£a-zA-Z]+(?:\s+[ê°€-í£a-zA-Z]+){1,3}', text)

        # ë¹ˆë„ ê¸°ë°˜ í•„í„°ë§
        phrase_freq = Counter(noun_phrases)
        related_concepts = [phrase for phrase, freq in phrase_freq.most_common(10) if freq >= 2]

        return related_concepts[:8]

    def _extract_domain_indicators(self, text: str) -> List[str]:
        """ë„ë©”ì¸ ì§€ì‹œì–´ ì¶”ì¶œ"""
        text_lower = text.lower()

        # ë„ë©”ì¸ë³„ ì§€ì‹œì–´ íŒ¨í„´
        domain_patterns = {
            'technical': [r'\b\w*api\w*\b', r'\b\w*system\w*\b', r'\b\w*data\w*\b'],
            'business': [r'\b\w*business\w*\b', r'\b\w*market\w*\b', r'\b\w*strategy\w*\b'],
            'legal': [r'\b\w*law\w*\b', r'\b\w*regulation\w*\b', r'\b\w*contract\w*\b'],
            'academic': [r'\b\w*research\w*\b', r'\b\w*study\w*\b', r'\b\w*analysis\w*\b']
        }

        indicators = []
        for domain, patterns in domain_patterns.items():
            for pattern in patterns:
                matches = re.findall(pattern, text_lower)
                if matches:
                    # setì„ listë¡œ ë³€í™˜í•œ í›„ ìŠ¬ë¼ì´ì‹±
                    unique_matches = list(set(matches))[:2]
                    indicators.extend([f"{domain}:{match}" for match in unique_matches])

        return indicators[:10]


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”ì¸ ì˜¨í†¨ë¡œì§€ ì¶”ì¶œê¸° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class OntologyExtractor:
    """í†µí•© ì˜¨í†¨ë¡œì§€ ì¶”ì¶œê¸°"""

    def __init__(self):
        self.keyword_extractor = KeywordExtractor()
        self.metadata_extractor = MetadataExtractor()
        self.context_extractor = ContextExtractor()

    def _merge_keywords(self, keywords_by_method: Dict[str, List[KeywordInfo]], top_k: int = 20) -> List[KeywordInfo]:
        """ë‹¤ì¤‘ ì¶”ì¶œê¸° ê²°ê³¼ë¥¼ í†µí•©"""
        merged = {}
        priority = ["keybert", "llm", "tfidf", "stat"]

        for method in priority:
            for kw in keywords_by_method.get(method, []):
                key = kw.term.lower()
                if key not in merged:
                    merged[key] = kw

        return list(merged.values())[:top_k]


    def extract_ontology(self, text: str, doc_id: str, source: str, chunks: List[str] = None,
                         keyword_methods: List[str] = ["keybert"]) -> OntologyResult:
        """ì „ì²´ ì˜¨í†¨ë¡œì§€ ì¶”ì¶œ"""
        import time
        start_time = time.time()

        logger.info(f"Starting ontology extraction for: {source}")

        try:
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í‚¤ì›Œë“œ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            keywords_start = time.time()

            # 1ì°¨ ì¶”ì¶œ: í†µê³„ ê¸°ë°˜ í‚¤ì›Œë“œ (ê¸°ì¡´ í‚¤ì›Œë“œ ëª©ë¡ìœ¼ë¡œ í™œìš©)
            fallback_keywords = self.keyword_extractor._extract_statistical(text, top_k=15)
            existing_terms = [kw.term for kw in fallback_keywords]

            # ì¶”ì¶œê¸°ë³„ ê²°ê³¼
            keywords_by_method = {}

            for method in keyword_methods:
                if method == "keybert":
                    keywords_by_method["keybert"] = self.keyword_extractor.extract_keywords(
                        text, existing_keywords=existing_terms
                    )
                elif method == "llm":
                    from .extractors.llm_keyword_extractor import LLMKeywordExtractor
                    llm_extractor = LLMKeywordExtractor()
                    keywords_by_method["llm"] = llm_extractor.extract_keywords(
                        text, existing_keywords=existing_terms
                    )

            merged_keywords = self._merge_keywords(keywords_by_method, top_k=20)
            keywords_time = time.time() - keywords_start

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            metadata_start = time.time()
            metadata = self.metadata_extractor.extract_metadata(text, source)
            metadata_time = time.time() - metadata_start

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            context_start = time.time()
            context = self.context_extractor.extract_context(text, chunks)
            context_time = time.time() - context_start

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê²°ê³¼ êµ¬ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            total_time = time.time() - start_time

            processing_stats = {
                'total_time': total_time,
                'keywords_time': keywords_time,
                'metadata_time': metadata_time,
                'context_time': context_time,
                'keywords_count': len(merged_keywords),
                'entities_count': len(metadata.key_entities),
                'topics_count': len(context.main_topics)
            }

            return OntologyResult(
                doc_id=doc_id,
                source=source,
                keywords=merged_keywords,
                metadata=metadata,
                context=context,
                extracted_at=datetime.now(),
                processing_stats=processing_stats
            )

        except Exception as e:
            logger.error(f"Ontology extraction failed: {e}")
            raise

    def to_dict(self, result: OntologyResult) -> Dict[str, Any]:
        """ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜"""
        return {
            'doc_id': result.doc_id,
            'source': result.source,
            'keywords': [asdict(kw) for kw in result.keywords],
            'metadata': asdict(result.metadata),
            'context': asdict(result.context),
            'extracted_at': result.extracted_at.isoformat(),
            'processing_stats': result.processing_stats
        }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def extract_ontology_from_chunks(chunks: List[Dict], doc_id: str, source: str) -> OntologyResult:
    """ì²­í¬ë“¤ë¡œë¶€í„° ì˜¨í†¨ë¡œì§€ ì¶”ì¶œ"""
    # ëª¨ë“  ì²­í¬ì˜ í…ìŠ¤íŠ¸ ê²°í•©
    full_text = '\n\n'.join(chunk['content'] for chunk in chunks if chunk.get('content'))
    chunk_texts = [chunk['content'] for chunk in chunks if chunk.get('content')]

    extractor = OntologyExtractor()
    return extractor.extract_ontology(full_text, doc_id, source, chunk_texts)


# í…ŒìŠ¤íŠ¸ìš© ë©”ì¸ í•¨ìˆ˜
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    test_text = """
    ì¸ê³µì§€ëŠ¥ ê¸°ë°˜ ë¬¸ì„œ ë¶„ì„ ì‹œìŠ¤í…œì€ ìì—°ì–´ ì²˜ë¦¬ ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ 
    ëŒ€ìš©ëŸ‰ ë¬¸ì„œì—ì„œ ì˜ë¯¸ ìˆëŠ” ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. 
    ì´ ì‹œìŠ¤í…œì€ KeyBERTì™€ spaCyë¥¼ ì‚¬ìš©í•˜ì—¬ í‚¤ì›Œë“œì™€ ê°œì²´ëª…ì„ ì¸ì‹í•˜ë©°,
    ì‚¬ìš©ìì—ê²Œ ì •í™•í•œ ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """

    extractor = OntologyExtractor()
    result = extractor.extract_ontology(test_text, "test-001", "test_document.txt")

    print("=== ì˜¨í†¨ë¡œì§€ ì¶”ì¶œ ê²°ê³¼ ===")
    print(f"í‚¤ì›Œë“œ ìˆ˜: {len(result.keywords)}")
    print(f"ê°œì²´ ìˆ˜: {len(result.metadata.key_entities)}")
    print(f"ì£¼ì œ ìˆ˜: {len(result.context.main_topics)}")

    print("\nì£¼ìš” í‚¤ì›Œë“œ:")
    for kw in result.keywords[:5]:
        print(f"  - {kw.term} (score: {kw.score:.3f}, freq: {kw.frequency})")

    print(f"\në¬¸ì„œ ìœ í˜•: {result.metadata.document_type}")
    print(f"ì¶”ì • ë„ë©”ì¸: {result.metadata.estimated_domain}")
    print(f"ì–¸ì–´: {result.metadata.language}")