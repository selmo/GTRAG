"""
Ontology API ëª¨ë¸ë“¤ - FastAPIìš© Pydantic ëª¨ë¸ ì •ì˜
extractor.pyì˜ dataclassë“¤ì„ API ì¹œí™”ì ìœ¼ë¡œ ë³€í™˜
"""
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
from enum import Enum

from pydantic import BaseModel, Field, validator
from pydantic.config import ConfigDict


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ê¸°ë³¸ ì—´ê±°í˜• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class KeywordCategory(str, Enum):
    """í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬"""
    TECHNICAL = "technical"
    PERSON = "person"
    ORGANIZATION = "organization"
    LOCATION = "location"
    GENERAL = "general"
    STATISTICAL = "statistical"


class DocumentType(str, Enum):
    """ë¬¸ì„œ ìœ í˜•"""
    CONTRACT = "contract"
    REPORT = "report"
    MANUAL = "manual"
    LEGAL = "legal"
    ACADEMIC = "academic"
    PROCEDURE = "procedure"
    GENERAL = "general"


class Domain(str, Enum):
    """ë„ë©”ì¸ ë¶„ë¥˜"""
    TECHNOLOGY = "technology"
    FINANCE = "finance"
    LEGAL = "legal"
    MEDICAL = "medical"
    BUSINESS = "business"
    ACADEMIC = "academic"
    GENERAL = "general"


class Language(str, Enum):
    """ì–¸ì–´"""
    KOREAN = "korean"
    ENGLISH = "english"
    MIXED = "mixed"
    UNKNOWN = "unknown"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•µì‹¬ ë°ì´í„° ëª¨ë¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class KeywordInfoModel(BaseModel):
    """í‚¤ì›Œë“œ ì •ë³´ ëª¨ë¸"""
    model_config = ConfigDict(from_attributes=True)

    term: str = Field(..., description="í‚¤ì›Œë“œ ìš©ì–´")
    score: float = Field(..., ge=0.0, le=1.0, description="ì¤‘ìš”ë„ ì ìˆ˜")
    frequency: int = Field(..., ge=1, description="ë¬¸ì„œ ë‚´ ì¶œí˜„ ë¹ˆë„")
    category: KeywordCategory = Field(..., description="í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬")
    positions: List[int] = Field(default_factory=list, description="ë¬¸ì„œ ë‚´ ìœ„ì¹˜ (ìµœëŒ€ 5ê°œ)")
    description: str = Field(default="", description="í‚¤ì›Œë“œ ì„¤ëª…")  # ğŸ” ì¶”ê°€

    @validator('positions')
    def validate_positions(cls, v):
        if len(v) > 5:
            return v[:5]  # ìµœëŒ€ 5ê°œë¡œ ì œí•œ
        return v


class EntityInfoModel(BaseModel):
    """ê°œì²´ëª… ì •ë³´ ëª¨ë¸"""
    model_config = ConfigDict(from_attributes=True)

    text: str = Field(..., description="ê°œì²´ëª… í…ìŠ¤íŠ¸")
    label: str = Field(..., description="ê°œì²´ëª… ë¼ë²¨ (PERSON, ORG, GPE ë“±)")
    start: int = Field(..., ge=0, description="ì‹œì‘ ìœ„ì¹˜")
    end: int = Field(..., ge=0, description="ì¢…ë£Œ ìœ„ì¹˜")
    confidence: float = Field(default=1.0, ge=0.0, le=1.0, description="ì‹ ë¢°ë„")

    @validator('end')
    def validate_end_position(cls, v, values):
        if 'start' in values and v <= values['start']:
            raise ValueError('end position must be greater than start position')
        return v


class TextStatisticsModel(BaseModel):
    """í…ìŠ¤íŠ¸ í†µê³„ ëª¨ë¸"""
    model_config = ConfigDict(from_attributes=True)

    total_length: int = Field(..., ge=0, description="ì´ ë¬¸ì ìˆ˜")
    lines: int = Field(..., ge=0, description="ì¤„ ìˆ˜")
    words: int = Field(..., ge=0, description="ë‹¨ì–´ ìˆ˜")
    sentences: int = Field(..., ge=0, description="ë¬¸ì¥ ìˆ˜")
    korean_chars: int = Field(..., ge=0, description="í•œê¸€ ë¬¸ì ìˆ˜")
    english_chars: int = Field(..., ge=0, description="ì˜ë¬¸ ë¬¸ì ìˆ˜")
    numbers: int = Field(..., ge=0, description="ìˆ«ì ê°œìˆ˜")
    avg_word_length: float = Field(..., ge=0, description="í‰ê·  ë‹¨ì–´ ê¸¸ì´")
    avg_sentence_length: float = Field(..., ge=0, description="í‰ê·  ë¬¸ì¥ ê¸¸ì´")


class StructureInfoModel(BaseModel):
    """ë¬¸ì„œ êµ¬ì¡° ì •ë³´ ëª¨ë¸"""
    model_config = ConfigDict(from_attributes=True)

    total_lines: int = Field(..., ge=0, description="ì´ ì¤„ ìˆ˜")
    empty_lines: int = Field(..., ge=0, description="ë¹ˆ ì¤„ ìˆ˜")
    potential_headers: int = Field(..., ge=0, description="ì œëª© ê°€ëŠ¥ ì¤„ ìˆ˜")
    list_items: int = Field(..., ge=0, description="ëª©ë¡ í•­ëª© ìˆ˜")
    has_numbered_sections: bool = Field(default=False, description="ë²ˆí˜¸ ì„¹ì…˜ ì—¬ë¶€")
    has_bullet_points: bool = Field(default=False, description="ë¶ˆë¦¿ í¬ì¸íŠ¸ ì—¬ë¶€")


class DocumentMetadataModel(BaseModel):
    """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ëª¨ë¸"""
    model_config = ConfigDict(from_attributes=True)

    language: Language = Field(..., description="ë¬¸ì„œ ì–¸ì–´")
    document_type: DocumentType = Field(..., description="ë¬¸ì„œ ìœ í˜•")
    estimated_domain: Domain = Field(..., description="ì¶”ì • ë„ë©”ì¸")
    key_entities: List[EntityInfoModel] = Field(default_factory=list, description="ì£¼ìš” ê°œì²´ëª…")
    text_statistics: TextStatisticsModel = Field(..., description="í…ìŠ¤íŠ¸ í†µê³„")
    structure_info: StructureInfoModel = Field(..., description="ë¬¸ì„œ êµ¬ì¡° ì •ë³´")


class SemanticClusterModel(BaseModel):
    """ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„° ëª¨ë¸"""
    model_config = ConfigDict(from_attributes=True)

    cluster_id: int = Field(..., ge=0, description="í´ëŸ¬ìŠ¤í„° ID")
    size: int = Field(..., ge=1, description="í´ëŸ¬ìŠ¤í„° í¬ê¸°")
    representative_chunk: str = Field(..., description="ëŒ€í‘œ ì²­í¬")
    avg_similarity: float = Field(..., ge=0.0, le=1.0, description="í‰ê·  ìœ ì‚¬ë„")
    chunk_indices: List[int] = Field(default_factory=list, description="í¬í•¨ëœ ì²­í¬ ì¸ë±ìŠ¤")


class ContextInfoModel(BaseModel):
    """ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ëª¨ë¸"""
    model_config = ConfigDict(from_attributes=True)

    main_topics: List[str] = Field(default_factory=list, description="ì£¼ìš” ì£¼ì œ")
    semantic_clusters: List[SemanticClusterModel] = Field(default_factory=list, description="ì˜ë¯¸ì  í´ëŸ¬ìŠ¤í„°")
    related_concepts: List[str] = Field(default_factory=list, description="ê´€ë ¨ ê°œë…")
    domain_indicators: List[str] = Field(default_factory=list, description="ë„ë©”ì¸ ì§€ì‹œì–´")


class ProcessingStatsModel(BaseModel):
    """ì²˜ë¦¬ í†µê³„ ëª¨ë¸"""
    model_config = ConfigDict(from_attributes=True)

    total_time: float = Field(..., ge=0, description="ì´ ì²˜ë¦¬ ì‹œê°„(ì´ˆ)")
    keywords_time: float = Field(..., ge=0, description="í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œê°„(ì´ˆ)")
    metadata_time: float = Field(..., ge=0, description="ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œê°„(ì´ˆ)")
    context_time: float = Field(..., ge=0, description="ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œê°„(ì´ˆ)")
    keywords_count: int = Field(..., ge=0, description="ì¶”ì¶œëœ í‚¤ì›Œë“œ ìˆ˜")
    entities_count: int = Field(..., ge=0, description="ì¶”ì¶œëœ ê°œì²´ëª… ìˆ˜")
    topics_count: int = Field(..., ge=0, description="ì¶”ì¶œëœ ì£¼ì œ ìˆ˜")


class OntologyResultModel(BaseModel):
    """ì˜¨í†¨ë¡œì§€ ì¶”ì¶œ ê²°ê³¼ ëª¨ë¸"""
    model_config = ConfigDict(from_attributes=True)

    doc_id: str = Field(..., description="ë¬¸ì„œ ID")
    source: str = Field(..., description="ì›ë³¸ íŒŒì¼ëª…")
    keywords: List[KeywordInfoModel] = Field(default_factory=list, description="ì¶”ì¶œëœ í‚¤ì›Œë“œ")
    metadata: DocumentMetadataModel = Field(..., description="ë¬¸ì„œ ë©”íƒ€ë°ì´í„°")
    context: ContextInfoModel = Field(..., description="ë¬¸ì„œ ì»¨í…ìŠ¤íŠ¸")
    extracted_at: datetime = Field(..., description="ì¶”ì¶œ ì‹œê°")
    processing_stats: ProcessingStatsModel = Field(..., description="ì²˜ë¦¬ í†µê³„")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ API ìš”ì²­ ëª¨ë¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class KeywordSearchRequest(BaseModel):
    """í‚¤ì›Œë“œ ê²€ìƒ‰ ìš”ì²­"""
    keyword: str = Field(..., min_length=1, max_length=100, description="ê²€ìƒ‰í•  í‚¤ì›Œë“œ")
    limit: int = Field(default=10, ge=1, le=100, description="ê²°ê³¼ ìˆ˜ ì œí•œ")
    min_score: float = Field(default=0.7, ge=0.0, le=1.0, description="ìµœì†Œ ìœ ì‚¬ë„ ì ìˆ˜")
    category: Optional[KeywordCategory] = Field(default=None, description="í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ í•„í„°")
    domain: Optional[Domain] = Field(default=None, description="ë„ë©”ì¸ í•„í„°")


class DomainSearchRequest(BaseModel):
    """ë„ë©”ì¸ë³„ ê²€ìƒ‰ ìš”ì²­"""
    domain: Domain = Field(..., description="ê²€ìƒ‰í•  ë„ë©”ì¸")
    limit: int = Field(default=20, ge=1, le=100, description="ê²°ê³¼ ìˆ˜ ì œí•œ")
    document_type: Optional[DocumentType] = Field(default=None, description="ë¬¸ì„œ ìœ í˜• í•„í„°")
    language: Optional[Language] = Field(default=None, description="ì–¸ì–´ í•„í„°")


class SimilarDocumentsRequest(BaseModel):
    """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ìš”ì²­"""
    doc_id: str = Field(..., description="ê¸°ì¤€ ë¬¸ì„œ ID")
    limit: int = Field(default=5, ge=1, le=20, description="ê²°ê³¼ ìˆ˜ ì œí•œ")
    min_similarity: float = Field(default=0.6, ge=0.0, le=1.0, description="ìµœì†Œ ìœ ì‚¬ë„")
    same_domain_only: bool = Field(default=False, description="ë™ì¼ ë„ë©”ì¸ë§Œ ê²€ìƒ‰")


class TopKeywordsRequest(BaseModel):
    """ìƒìœ„ í‚¤ì›Œë“œ ì¡°íšŒ ìš”ì²­"""
    limit: int = Field(default=50, ge=1, le=200, description="ê²°ê³¼ ìˆ˜ ì œí•œ")
    category: Optional[KeywordCategory] = Field(default=None, description="ì¹´í…Œê³ ë¦¬ í•„í„°")
    domain: Optional[Domain] = Field(default=None, description="ë„ë©”ì¸ í•„í„°")
    min_doc_count: int = Field(default=1, ge=1, description="ìµœì†Œ ë¬¸ì„œ ì¶œí˜„ ìˆ˜")
    sort_by: str = Field(default="document_count", pattern="^(document_count|total_frequency|avg_score)$",
                        description="ì •ë ¬ ê¸°ì¤€")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ API ì‘ë‹µ ëª¨ë¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class KeywordSearchResult(BaseModel):
    """í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼"""
    keyword: str = Field(..., description="ì°¾ì€ í‚¤ì›Œë“œ")
    score: float = Field(..., description="ìœ ì‚¬ë„ ì ìˆ˜")
    doc_id: str = Field(..., description="ë¬¸ì„œ ID")
    source: str = Field(..., description="ì›ë³¸ íŒŒì¼ëª…")
    category: KeywordCategory = Field(..., description="í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬")
    document_type: DocumentType = Field(..., description="ë¬¸ì„œ ìœ í˜•")
    estimated_domain: Domain = Field(..., description="ì¶”ì • ë„ë©”ì¸")


class DocumentSummary(BaseModel):
    """ë¬¸ì„œ ìš”ì•½ ì •ë³´"""
    doc_id: str = Field(..., description="ë¬¸ì„œ ID")
    source: str = Field(..., description="ì›ë³¸ íŒŒì¼ëª…")
    document_type: DocumentType = Field(..., description="ë¬¸ì„œ ìœ í˜•")
    estimated_domain: Domain = Field(..., description="ì¶”ì • ë„ë©”ì¸")
    language: Language = Field(..., description="ì–¸ì–´")
    keyword_count: int = Field(..., description="í‚¤ì›Œë“œ ìˆ˜")
    entity_count: int = Field(..., description="ê°œì²´ëª… ìˆ˜")
    top_keywords: List[str] = Field(default_factory=list, description="ìƒìœ„ í‚¤ì›Œë“œ")
    main_topics: List[str] = Field(default_factory=list, description="ì£¼ìš” ì£¼ì œ")
    extracted_at: datetime = Field(..., description="ì¶”ì¶œ ì‹œê°")


class SimilarDocumentResult(BaseModel):
    """ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ê²°ê³¼"""
    doc_id: str = Field(..., description="ë¬¸ì„œ ID")
    source: str = Field(..., description="ì›ë³¸ íŒŒì¼ëª…")
    similarity_score: float = Field(..., description="ìœ ì‚¬ë„ ì ìˆ˜")
    document_type: DocumentType = Field(..., description="ë¬¸ì„œ ìœ í˜•")
    estimated_domain: Domain = Field(..., description="ì¶”ì • ë„ë©”ì¸")
    top_keywords: List[str] = Field(default_factory=list, description="ìƒìœ„ í‚¤ì›Œë“œ")
    main_topics: List[str] = Field(default_factory=list, description="ì£¼ìš” ì£¼ì œ")


class TopKeywordResult(BaseModel):
    """ìƒìœ„ í‚¤ì›Œë“œ ê²°ê³¼"""
    keyword: str = Field(..., description="í‚¤ì›Œë“œ")
    total_frequency: int = Field(..., description="ì´ ì¶œí˜„ ë¹ˆë„")
    avg_score: float = Field(..., description="í‰ê·  ì¤‘ìš”ë„ ì ìˆ˜")
    document_count: int = Field(..., description="ì¶œí˜„ ë¬¸ì„œ ìˆ˜")
    categories: List[KeywordCategory] = Field(default_factory=list, description="ì¹´í…Œê³ ë¦¬ë“¤")
    domains: List[Domain] = Field(default_factory=list, description="ë„ë©”ì¸ë“¤")
    sample_documents: List[str] = Field(default_factory=list, description="ìƒ˜í”Œ ë¬¸ì„œë“¤")


class OntologyStatistics(BaseModel):
    """ì˜¨í†¨ë¡œì§€ í†µê³„"""
    total_documents: int = Field(..., description="ì´ ë¬¸ì„œ ìˆ˜")
    total_keywords: int = Field(..., description="ì´ í‚¤ì›Œë“œ ìˆ˜")
    domain_distribution: Dict[str, int] = Field(default_factory=dict, description="ë„ë©”ì¸ë³„ ë¶„í¬")
    document_type_distribution: Dict[str, int] = Field(default_factory=dict, description="ë¬¸ì„œ ìœ í˜•ë³„ ë¶„í¬")
    language_distribution: Dict[str, int] = Field(default_factory=dict, description="ì–¸ì–´ë³„ ë¶„í¬")
    keyword_category_distribution: Dict[str, int] = Field(default_factory=dict, description="í‚¤ì›Œë“œ ì¹´í…Œê³ ë¦¬ë³„ ë¶„í¬")
    avg_keywords_per_doc: float = Field(..., description="ë¬¸ì„œë‹¹ í‰ê·  í‚¤ì›Œë“œ ìˆ˜")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°°ì¹˜ ì²˜ë¦¬ ëª¨ë¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class BatchExtractionRequest(BaseModel):
    """ë°°ì¹˜ ì¶”ì¶œ ìš”ì²­"""
    doc_ids: List[str] = Field(..., min_items=1, max_items=50, description="ë¬¸ì„œ ID ë¦¬ìŠ¤íŠ¸")
    force_reextract: bool = Field(default=False, description="ê¸°ì¡´ ê²°ê³¼ ë¬´ì‹œí•˜ê³  ì¬ì¶”ì¶œ")


class BatchExtractionResult(BaseModel):
    """ë°°ì¹˜ ì¶”ì¶œ ê²°ê³¼"""
    total_requested: int = Field(..., description="ìš”ì²­ëœ ë¬¸ì„œ ìˆ˜")
    successful: int = Field(..., description="ì„±ê³µí•œ ë¬¸ì„œ ìˆ˜")
    failed: int = Field(..., description="ì‹¤íŒ¨í•œ ë¬¸ì„œ ìˆ˜")
    skipped: int = Field(..., description="ê±´ë„ˆë›´ ë¬¸ì„œ ìˆ˜")
    processing_time: float = Field(..., description="ì´ ì²˜ë¦¬ ì‹œê°„(ì´ˆ)")
    failed_doc_ids: List[str] = Field(default_factory=list, description="ì‹¤íŒ¨í•œ ë¬¸ì„œ IDë“¤")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„¤ì • ëª¨ë¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ExtractionConfig(BaseModel):
    """ì¶”ì¶œ ì„¤ì •"""
    max_keywords: int = Field(default=20, ge=5, le=100, description="ìµœëŒ€ í‚¤ì›Œë“œ ìˆ˜")
    keyword_score_threshold: float = Field(default=0.1, ge=0.0, le=1.0, description="í‚¤ì›Œë“œ ì ìˆ˜ ì„ê³„ê°’")
    use_keybert: bool = Field(default=True, description="KeyBERT ì‚¬ìš© ì—¬ë¶€")
    use_spacy: bool = Field(default=True, description="spaCy ì‚¬ìš© ì—¬ë¶€")
    chunk_size: int = Field(default=300, ge=100, le=1000, description="ì²­í¬ í¬ê¸°")
    max_topics: int = Field(default=5, ge=3, le=10, description="ìµœëŒ€ ì£¼ì œ ìˆ˜")


class SystemHealthModel(BaseModel):
    """ì‹œìŠ¤í…œ ìƒíƒœ ëª¨ë¸"""
    ontology_collections_status: Dict[str, bool] = Field(default_factory=dict, description="ì»¬ë ‰ì…˜ ìƒíƒœ")
    embedding_model_status: bool = Field(..., description="ì„ë² ë”© ëª¨ë¸ ìƒíƒœ")
    keybert_available: bool = Field(..., description="KeyBERT ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€")
    spacy_available: bool = Field(..., description="spaCy ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€")
    total_documents: int = Field(..., description="ì´ ì˜¨í†¨ë¡œì§€ ë¬¸ì„œ ìˆ˜")
    total_keywords: int = Field(..., description="ì´ í‚¤ì›Œë“œ ìˆ˜")
    last_extraction: Optional[datetime] = Field(default=None, description="ë§ˆì§€ë§‰ ì¶”ì¶œ ì‹œê°„")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì—ëŸ¬ ëª¨ë¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class OntologyError(BaseModel):
    """ì˜¨í†¨ë¡œì§€ ì˜¤ë¥˜ ëª¨ë¸"""
    error_code: str = Field(..., description="ì˜¤ë¥˜ ì½”ë“œ")
    error_message: str = Field(..., description="ì˜¤ë¥˜ ë©”ì‹œì§€")
    error_details: Optional[Dict[str, Any]] = Field(default=None, description="ì˜¤ë¥˜ ìƒì„¸ ì •ë³´")
    timestamp: datetime = Field(default_factory=datetime.now, description="ì˜¤ë¥˜ ë°œìƒ ì‹œê°„")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì„±ê³µ ì‘ë‹µ ëª¨ë¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class SuccessResponse(BaseModel):
    """ì„±ê³µ ì‘ë‹µ"""
    success: bool = Field(default=True, description="ì„±ê³µ ì—¬ë¶€")
    message: str = Field(..., description="ì„±ê³µ ë©”ì‹œì§€")
    data: Optional[Any] = Field(default=None, description="ì‘ë‹µ ë°ì´í„°")


class PaginatedResponse(BaseModel):
    """í˜ì´ì§€ë„¤ì´ì…˜ ì‘ë‹µ"""
    items: List[Any] = Field(..., description="ê²°ê³¼ ì•„ì´í…œë“¤")
    total: int = Field(..., description="ì´ ê²°ê³¼ ìˆ˜")
    page: int = Field(..., description="í˜„ì¬ í˜ì´ì§€")
    per_page: int = Field(..., description="í˜ì´ì§€ë‹¹ ì•„ì´í…œ ìˆ˜")
    has_next: bool = Field(..., description="ë‹¤ìŒ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€")
    has_prev: bool = Field(..., description="ì´ì „ í˜ì´ì§€ ì¡´ì¬ ì—¬ë¶€")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def convert_ontology_result_to_model(result) -> OntologyResultModel:
    """extractorì˜ OntologyResultë¥¼ Pydantic ëª¨ë¸ë¡œ ë³€í™˜"""
    from .extractor import OntologyResult

    if isinstance(result, OntologyResult):
        return OntologyResultModel(
            doc_id=result.doc_id,
            source=result.source,
            keywords=[
                KeywordInfoModel(
                    term=kw.term,
                    score=kw.score,
                    frequency=kw.frequency,
                    category=kw.category,
                    positions=kw.positions
                ) for kw in result.keywords
            ],
            metadata=DocumentMetadataModel(
                language=result.metadata.language,
                document_type=result.metadata.document_type,
                estimated_domain=result.metadata.estimated_domain,
                key_entities=[
                    EntityInfoModel(
                        text=ent.text,
                        label=ent.label,
                        start=ent.start,
                        end=ent.end,
                        confidence=ent.confidence
                    ) for ent in result.metadata.key_entities
                ],
                text_statistics=TextStatisticsModel(**result.metadata.text_statistics),
                structure_info=StructureInfoModel(**result.metadata.structure_info)
            ),
            context=ContextInfoModel(
                main_topics=result.context.main_topics,
                semantic_clusters=[
                    SemanticClusterModel(**cluster) for cluster in result.context.semantic_clusters
                ],
                related_concepts=result.context.related_concepts,
                domain_indicators=result.context.domain_indicators
            ),
            extracted_at=result.extracted_at,
            processing_stats=ProcessingStatsModel(**result.processing_stats)
        )

    raise ValueError("Invalid result type for conversion")


# íƒ€ì… ë³„ì¹­
OntologyData = Union[OntologyResultModel, DocumentSummary]
SearchResults = List[Union[KeywordSearchResult, SimilarDocumentResult]]