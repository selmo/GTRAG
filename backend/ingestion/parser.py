"""
ê°œì„ ëœ ë¬¸ì„œ íŒŒì„œ - í•œêµ­ì–´ PDF ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
"""
import os
import tempfile
from uuid import uuid4
from typing import List, Dict, Union
import logging
import re
from ftfy import fix_text


logger = logging.getLogger(__name__)

def clean_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì¸ì½”ë”© ë¬¸ì œ í•´ê²°"""
    if not text:
        return ""

    # ë°”ì´íŠ¸ ë¬¸ìì—´ì´ë©´ UTF-8ë¡œ ë””ì½”ë“œ
    if isinstance(text, bytes):
        try:
            text = text.decode('utf-8')
        except UnicodeDecodeError:
            try:
                text = text.decode('cp949')  # í•œêµ­ì–´ ìœˆë„ìš° ì¸ì½”ë”©
            except UnicodeDecodeError:
                text = text.decode('utf-8', errors='replace')

    # â‘  ftfy ë¡œ í•œ ë²ˆ ë³µêµ¬
    text = fix_text(text)

    # â‘¡ ì œì–´ ë¬¸ì ì œê±° (ì¤„ë°”ê¿ˆ, íƒ­ ì œì™¸)
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)

    # ì´ìƒí•œ ë¬¸ìë“¤ ì •ë¦¬ (PDFì—ì„œ ìì£¼ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ì œë“¤)
    text = re.sub(r'[^\w\sê°€-í£ã„±-ã…ã…-ã…£ä¸€-é¾¯\u3040-\u309F\u30A0-\u30FF.,!?;:()\[\]{}\'\"%-]', ' ', text)

    # ì—°ì†ëœ ê³µë°± ì •ë¦¬
    text = re.sub(r'\s+', ' ', text)

    # ì•ë’¤ ê³µë°± ì œê±°
    text = text.strip()

    return text


GARBLED_THRESHOLD = 0.30          # ìœ íš¨ ë¬¸ì ë¹„ìœ¨ 30 %

def is_garbled(text: str) -> bool:
    """ê°€ë… ê°€ëŠ¥í•œ ë¬¸ì ë¹„ìœ¨ì´ ì„ê³„ê°’ë³´ë‹¤ ë‚®ìœ¼ë©´ True"""
    if not text:
        return True
    good = len(re.findall(r'[ê°€-í£a-zA-Z0-9]', text))
    return good / len(text) < GARBLED_THRESHOLD


def parse_pdf_with_pypdf(file_path: str, lang_hint="auto") -> List[Dict]:
    """PyPDFë¥¼ ì‚¬ìš©í•œ PDF íŒŒì‹± (ì¸ì½”ë”© ê°œì„ )"""
    try:
        from pypdf import PdfReader

        chunks = []

        # ë‹¤ì–‘í•œ ì¸ì½”ë”©ìœ¼ë¡œ íŒŒì¼ ì½ê¸° ì‹œë„
        for encoding in ['utf-8', 'latin-1', 'cp1252']:
            try:
                reader = PdfReader(file_path)
                break
            except Exception as e:
                logger.warning(f"Failed to read PDF with encoding {encoding}: {e}")
                continue
        else:
            raise Exception("Could not read PDF with any encoding")

        for page_num, page in enumerate(reader.pages):
            try:
                # í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = page.extract_text()

                if not text:
                    logger.warning(f"No text extracted from page {page_num + 1}")
                    continue

                # í…ìŠ¤íŠ¸ ì •ë¦¬
                cleaned_text = clean_text(text)

                if cleaned_text and len(cleaned_text.strip()) > 10:
                    chunks.extend(chunk_text(
                        cleaned_text,
                        source=os.path.basename(file_path),
                        page=page_num + 1,
                        doc_type="pdf",
                        lang=lang_hint
                    ))

            except Exception as e:
                logger.warning(f"Failed to extract text from page {page_num + 1}: {e}")
                continue

        logger.info(f"PDF parsed with PyPDF: {len(chunks)} chunks from {len(reader.pages)} pages")
        return chunks

    except ImportError:
        logger.error("PyPDF not available")
        return parse_as_fallback(file_path, "PDF ë¬¸ì„œ (PyPDF ë¯¸ì„¤ì¹˜)")
    except Exception as e:
        logger.error(f"PDF parsing failed with PyPDF: {e}")
        # pdfplumberë¡œ fallback
        return parse_pdf_with_pdfplumber(file_path, lang_hint)


def parse_pdf_with_pdfplumber(file_path: str, lang_hint="auto") -> List[Dict]:
    """pdfplumberë¥¼ ì‚¬ìš©í•œ PDF íŒŒì‹± (í•œêµ­ì–´ ìµœì í™”)"""
    try:
        import pdfplumber

        chunks = []

        # pdfplumber ì„¤ì • - CJK í°íŠ¸ë¥¼ ìœ„í•œ íŠ¹ë³„ ì²˜ë¦¬
        pdf_config = {
            'dedupe_chars': True,  # ì¤‘ë³µ ë¬¸ì ì œê±°
            'ignore_blank_chars': True,  # ë¹ˆ ë¬¸ì ë¬´ì‹œ
            'use_text_flow': True,  # í…ìŠ¤íŠ¸ íë¦„ ì‚¬ìš©
        }

        with pdfplumber.open(file_path, **pdf_config) as pdf:
            for page_num, page in enumerate(pdf.pages):
                try:
                    # ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                    text = None

                    # ë°©ë²• 1: ê¸°ë³¸ ì¶”ì¶œ
                    try:
                        text = page.extract_text()
                    except Exception as e:
                        logger.warning(f"Default text extraction failed on page {page_num + 1}: {e}")

                    # ë°©ë²• 2: ë” ê´€ëŒ€í•œ ì„¤ì •ìœ¼ë¡œ ì¶”ì¶œ
                    if not text or len(text.strip()) < 10:
                        try:
                            text = page.extract_text(
                                x_tolerance=3,  # ë¬¸ì ê°„ ê°„ê²© í—ˆìš©ë„
                                y_tolerance=3,  # ì¤„ ê°„ ê°„ê²© í—ˆìš©ë„
                                layout=True,    # ë ˆì´ì•„ì›ƒ ìœ ì§€
                                x_density=7.25, # í•´ìƒë„ ì„¤ì •
                                y_density=7.25
                            )
                        except Exception as e:
                            logger.warning(f"Alternative text extraction failed on page {page_num + 1}: {e}")

                    # ë°©ë²• 3: ë¬¸ì ê¸°ë°˜ ì¶”ì¶œ (ê°€ì¥ ì•ˆì „í•¨)
                    if not text or len(text.strip()) < 10:
                        try:
                            chars = page.chars
                            if chars:
                                # ë¬¸ìë¥¼ ìœ„ì¹˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬
                                sorted_chars = sorted(chars, key=lambda x: (x['top'], x['x0']))
                                text = ''.join(char['text'] for char in sorted_chars)
                        except Exception as e:
                            logger.warning(f"Char-based text extraction failed on page {page_num + 1}: {e}")

                    if not text:
                        logger.warning(f"No text extracted from page {page_num + 1}")
                        continue

                    # í…ìŠ¤íŠ¸ ì •ë¦¬
                    cleaned_text = clean_text(text)

                    if cleaned_text and len(cleaned_text.strip()) > 10:
                        chunks.extend(chunk_text(
                            cleaned_text,
                            source=os.path.basename(file_path),
                            page=page_num + 1,
                            doc_type="pdf",
                            lang=lang_hint
                        ))

                except Exception as e:
                    logger.warning(f"Failed to extract text from page {page_num + 1}: {e}")
                    continue

        logger.info(f"PDF parsed with pdfplumber: {len(chunks)} chunks")
        return chunks

    except ImportError:
        logger.error("pdfplumber not available")
        return parse_as_fallback(file_path, "PDF ë¬¸ì„œ (pdfplumber ë¯¸ì„¤ì¹˜)")
    except Exception as e:
        logger.error(f"PDF parsing failed with pdfplumber: {e}")
        # PyMuPDFë¡œ fallback
        return parse_pdf_with_pymupdf(file_path, lang_hint)


def parse_pdf_with_pymupdf(file_path: str, lang_hint="auto") -> List[Dict]:
    """PyMuPDFë¥¼ ì‚¬ìš©í•œ PDF íŒŒì‹± (í•œêµ­ì–´ ìµœì í™”)"""
    try:
        import fitz  # PyMuPDF

        chunks = []
        doc = fitz.open(file_path)

        for page_num in range(len(doc)):
            try:
                page = doc.load_page(page_num)

                # í…ìŠ¤íŠ¸ ì¶”ì¶œ - CJK ë¬¸ìë¥¼ ìœ„í•œ í”Œë˜ê·¸ ì„¤ì •
                text = page.get_text(
                    "text",
                    flags=fitz.TEXT_PRESERVE_WHITESPACE | fitz.TEXT_PRESERVE_SPANS
                )

                # ì¶”ê°€ì ì¸ ì¶”ì¶œ ë°©ë²• ì‹œë„
                if not text or len(text.strip()) < 10:
                    # dict í˜•íƒœë¡œ ì¶”ì¶œí•´ì„œ í°íŠ¸ ì •ë³´ê¹Œì§€ í™œìš©
                    text_dict = page.get_text("dict")
                    extracted_text = []

                    for block in text_dict["blocks"]:
                        if "lines" in block:
                            for line in block["lines"]:
                                for span in line["spans"]:
                                    if span["text"].strip():
                                        extracted_text.append(span["text"])

                    text = " ".join(extracted_text)

                if not text:
                    logger.warning(f"No text extracted from page {page_num + 1}")
                    continue

                # í…ìŠ¤íŠ¸ ì •ë¦¬
                cleaned_text = clean_text(text)

                if cleaned_text and len(cleaned_text.strip()) > 10:
                    chunks.extend(chunk_text(
                        cleaned_text,
                        source=os.path.basename(file_path),
                        page=page_num + 1,
                        doc_type="pdf",
                        lang=lang_hint
                    ))

            except Exception as e:
                logger.warning(f"Failed to extract text from page {page_num + 1}: {e}")
                continue

        doc.close()
        logger.info(f"PDF parsed with PyMuPDF: {len(chunks)} chunks")
        return chunks

    except ImportError:
        logger.error("PyMuPDF not available")
        return parse_as_fallback(file_path, "PDF ë¬¸ì„œ (PyMuPDF ë¯¸ì„¤ì¹˜)")
    except Exception as e:
        logger.error(f"PDF parsing failed with PyMuPDF: {e}")
        return parse_as_fallback(file_path, f"PDF íŒŒì‹± ì˜¤ë¥˜: {str(e)}")


def parse_text_file(file_path: str, lang_hint="auto") -> List[Dict]:
    """í…ìŠ¤íŠ¸ íŒŒì¼ íŒŒì‹± (ì¸ì½”ë”© ê°œì„ )"""
    try:
        # ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„
        encodings = ['utf-8', 'utf-8-sig', 'cp949', 'euc-kr', 'latin-1', 'utf-16', 'ascii']
        content = None
        used_encoding = None

        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read()
                    used_encoding = encoding
                break
            except (UnicodeDecodeError, UnicodeError):
                continue

        if content is None:
            # ë°”ì´ë„ˆë¦¬ë¡œ ì½ì–´ì„œ ê°•ì œ ë³€í™˜
            with open(file_path, 'rb') as f:
                raw_content = f.read()
                # chardetë¡œ ì¸ì½”ë”© ê°ì§€ ì‹œë„
                try:
                    import chardet
                    detected = chardet.detect(raw_content)
                    if detected and detected['encoding']:
                        content = raw_content.decode(detected['encoding'], errors='replace')
                        used_encoding = detected['encoding']
                    else:
                        content = raw_content.decode('utf-8', errors='replace')
                        used_encoding = 'utf-8 (forced)'
                except ImportError:
                    content = raw_content.decode('utf-8', errors='replace')
                    used_encoding = 'utf-8 (forced)'

        # í…ìŠ¤íŠ¸ ì •ë¦¬
        cleaned_content = clean_text(content)

        logger.info(f"Text file parsed with encoding: {used_encoding}")

        return chunk_text(
            cleaned_content,
            source=os.path.basename(file_path),
            doc_type="text",
            lang=lang_hint
        )

    except Exception as e:
        logger.error(f"Text file parsing failed: {e}")
        return parse_as_fallback(file_path, f"í…ìŠ¤íŠ¸ íŒŒì¼ íŒŒì‹± ì‹¤íŒ¨: {str(e)}")


def parse_docx(file_path: str, lang_hint="auto") -> List[Dict]:
    """Word ë¬¸ì„œ íŒŒì‹± (ì¸ì½”ë”© ê°œì„ )"""
    try:
        from docx import Document

        doc = Document(file_path)
        paragraphs = []

        # ë¬¸ë‹¨ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        for para in doc.paragraphs:
            if para.text.strip():
                cleaned_text = clean_text(para.text.strip())
                if cleaned_text:
                    paragraphs.append(cleaned_text)

        # í‘œ ë‚´ìš© ì¶”ì¶œ
        for table in doc.tables:
            for row in table.rows:
                row_texts = []
                for cell in row.cells:
                    cell_text = clean_text(cell.text.strip())
                    if cell_text:
                        row_texts.append(cell_text)

                if row_texts:
                    row_text = " | ".join(row_texts)
                    paragraphs.append(row_text)

        # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
        full_text = "\n\n".join(paragraphs)

        return chunk_text(
            full_text,
            source=os.path.basename(file_path),
            doc_type="docx",
            lang=lang_hint
        )

    except ImportError:
        logger.error("python-docx not available")
        return parse_as_fallback(file_path, "Word ë¬¸ì„œ (python-docx ë¯¸ì„¤ì¹˜)")
    except Exception as e:
        logger.error(f"DOCX parsing failed: {e}")
        return parse_as_fallback(file_path, f"Word ë¬¸ì„œ íŒŒì‹± ì˜¤ë¥˜: {str(e)}")


def parse_as_fallback(file_path: str, description: str) -> List[Dict]:
    """í´ë°± ì²˜ë¦¬"""
    return [{
        "chunk_id": str(uuid4()),
        "content": f"{description}ê°€ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {os.path.basename(file_path)}",
        "meta": {
            "source": os.path.basename(file_path),
            "type": "fallback",
            "note": description,
            "file_size": os.path.getsize(file_path) if os.path.exists(file_path) else 0
        }
    }]


def chunk_text(text: str, source: str, doc_type: str = "unknown",
               page: int = None, lang: str = "auto", chunk_size: int = 500,
               chunk_overlap: int = 50) -> List[Dict]:
    """í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í•  (í•œêµ­ì–´ ìµœì í™”)"""
    if not text or not text.strip():
        return []

    # í…ìŠ¤íŠ¸ ì •ë¦¬
    cleaned_text = clean_text(text)

    if not cleaned_text:
        return []

    chunks = []

    if len(cleaned_text) <= chunk_size:
        chunks.append({
            "chunk_id": str(uuid4()),
            "content": cleaned_text,
            "meta": {
                "source": source,
                "type": doc_type,
                "lang": lang,
                **({"page": page} if page else {}),
                "chunk_index": 0,
                "total_chunks": 1
            }
        })
    else:
        start = 0
        chunk_index = 0

        while start < len(cleaned_text):
            end = start + chunk_size

            # í•œêµ­ì–´ ë° ì˜ì–´ë¥¼ ê³ ë ¤í•œ ë‹¨ì–´ ê²½ê³„ì—ì„œ ìë¥´ê¸°
            if end < len(cleaned_text):
                # í•œêµ­ì–´ ë¬¸ì¥ ë ë¬¸ìë“¤
                korean_endings = ['ë‹¤', 'ìš”', 'ë‹ˆë‹¤', 'ìŠµë‹ˆë‹¤', 'ì£ ', 'ë„¤', 'ë¼', 'ê¹Œ', 'ì•¼']

                # ì°¾ì„ ë²”ìœ„ ì„¤ì •
                search_start = max(start + chunk_size // 2, end - 100)

                # ìš°ì„ ìˆœìœ„ 1: ë¬¸ì¥ ë¶€í˜¸
                for i in range(end, search_start, -1):
                    if cleaned_text[i] in ['.', '!', '?', 'ã€‚', '!', '?', '\n']:
                        end = i + 1
                        break
                else:
                    # ìš°ì„ ìˆœìœ„ 2: í•œêµ­ì–´ ë¬¸ì¥ ë
                    for i in range(end, search_start, -1):
                        if i > 0 and cleaned_text[i-1:i+1] in ['ë‹¤.', 'ìš”.', 'ë‹¤!', 'ìš”!', 'ë‹¤?', 'ìš”?']:
                            end = i + 1
                            break
                        elif cleaned_text[i] in korean_endings and i < len(cleaned_text) - 1 and cleaned_text[i+1] in [' ', '\n', '\t']:
                            end = i + 1
                            break
                    else:
                        # ìš°ì„ ìˆœìœ„ 3: ê³µë°±
                        for i in range(end, search_start, -1):
                            if cleaned_text[i] in [' ', '\n', '\t']:
                                end = i + 1
                                break

            chunk_content = cleaned_text[start:end].strip()

            if chunk_content and len(chunk_content) > 10:
                chunks.append({
                    "chunk_id": str(uuid4()),
                    "content": chunk_content,
                    "meta": {
                        "source": source,
                        "type": doc_type,
                        "lang": lang,
                        **({"page": page} if page else {}),
                        "chunk_index": chunk_index,
                        "start_pos": start,
                        "end_pos": end
                    }
                })
                chunk_index += 1

            start = max(start + chunk_size - chunk_overlap, end)

    # ì´ ì²­í¬ ìˆ˜ ì—…ë°ì´íŠ¸
    for chunk in chunks:
        chunk["meta"]["total_chunks"] = len(chunks)

    return chunks


def parse_pdf(file_input: Union[str, bytes], lang_hint: str = "auto") -> List[Dict]:
    """ë©”ì¸ íŒŒì‹± í•¨ìˆ˜ - ì—¬ëŸ¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ë‹¨ê³„ì  ì‹œë„"""
    # ë°”ì´íŠ¸ ë°ì´í„°ì¸ ê²½ìš° ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
    if isinstance(file_input, bytes):
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
            tmp_file.write(file_input)
            temp_path = tmp_file.name

        try:
            return parse_file_by_extension(temp_path, lang_hint)
        finally:
            try:
                os.unlink(temp_path)
            except:
                pass

    elif isinstance(file_input, (str, os.PathLike)):
        return parse_file_by_extension(str(file_input), lang_hint)

    else:
        raise ValueError(f"Unsupported input type: {type(file_input)}")


def parse_file_by_extension(file_path: str, lang_hint: str = "auto") -> List[Dict]:
    """íŒŒì¼ í™•ì¥ìì— ë”°ë¥¸ íŒŒì‹± - í•œêµ­ì–´ PDFì— ìµœì í™”ëœ ë‹¨ê³„ì  ì‹œë„"""
    file_ext = os.path.splitext(file_path)[1].lower()

    if file_ext == '.pdf':
        # PDF íŒŒì‹± - ì—¬ëŸ¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„
        successful_chunks = []

        # 1ë‹¨ê³„: pdfplumber ë¨¼ì € ì‹œë„ (í•œêµ­ì–´ì— ê°€ì¥ ì¢‹ìŒ)
        try:
            result = parse_pdf_with_pdfplumber(file_path, lang_hint)
            # if result and any(len(chunk['content'].strip()) > 50 for chunk in result if chunk['meta']['type'] != 'fallback'):
            if result and not is_garbled(" ".join(c['content'] for c in result)):
                successful_chunks = result
                logger.info(f"Successfully parsed with pdfplumber: {len(successful_chunks)} chunks")
                return successful_chunks
        except Exception as e:
            logger.warning(f"pdfplumber failed: {e}")

        # 2ë‹¨ê³„: PyMuPDF ì‹œë„
        if not successful_chunks:
            try:
                result = parse_pdf_with_pymupdf(file_path, lang_hint)
                # if result and any(len(chunk['content'].strip()) > 50 for chunk in result if chunk['meta']['type'] != 'fallback'):
                if result and not is_garbled(" ".join(c['content'] for c in result)):
                    successful_chunks = result
                    logger.info(f"Successfully parsed with PyMuPDF: {len(successful_chunks)} chunks")
                    return successful_chunks
            except Exception as e:
                logger.warning(f"PyMuPDF failed: {e}")

        # 3ë‹¨ê³„: PyPDF ì‹œë„
        if not successful_chunks:
            try:
                result = parse_pdf_with_pypdf(file_path, lang_hint)
                # if result and any(len(chunk['content'].strip()) > 50 for chunk in result if chunk['meta']['type'] != 'fallback'):
                if result and not is_garbled(" ".join(c['content'] for c in result)):
                    successful_chunks = result
                    logger.info(f"Successfully parsed with PyPDF: {len(successful_chunks)} chunks")
                    return successful_chunks
            except Exception as e:
                logger.warning(f"PyPDF failed: {e}")

        # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•œ ê²½ìš°
        if not successful_chunks:
            logger.error("All PDF parsing libraries failed")
            return parse_as_fallback(file_path, "PDF ë¬¸ì„œ (ëª¨ë“  íŒŒì‹± ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹¤íŒ¨)")

        return successful_chunks

    elif file_ext in ['.docx', '.doc']:
        return parse_docx(file_path, lang_hint)
    elif file_ext in ['.txt', '.md', '.rst']:
        return parse_text_file(file_path, lang_hint)
    else:
        # ê¸°ë³¸ì ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬ ì‹œë„
        try:
            return parse_text_file(file_path, lang_hint)
        except:
            return parse_as_fallback(file_path, f"ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼ í˜•ì‹ ({file_ext})")


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
if __name__ == "__main__":
    import sys

    print("ğŸ“„ Improved Korean PDF Parser Test")
    print("==================================")

    # ì¸ì½”ë”© í…ŒìŠ¤íŠ¸
    test_korean = "ì•ˆë…•í•˜ì„¸ìš”. ì´ê²ƒì€ í•œê¸€ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. AI ê¸°ë°˜ ê²€ìƒ‰ íŒ¨ëŸ¬ë‹¤ì„ ì „í™˜ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤."
    cleaned = clean_text(test_korean)
    print(f"âœ… Encoding test passed: {cleaned[:50]}...")

    if len(sys.argv) > 1:
        test_file = sys.argv[1]
        print(f"\nğŸ” Testing with: {test_file}")

        try:
            chunks = parse_pdf(test_file)
            print(f"âœ… Generated {len(chunks)} chunks")

            for i, chunk in enumerate(chunks[:3]):  # ì²˜ìŒ 3ê°œ ì²­í¬ë§Œ í‘œì‹œ
                print(f"\n--- Chunk {i+1} ---")
                print(f"Content preview: {chunk['content'][:200]}...")
                print(f"Content length: {len(chunk['content'])}")
                print(f"Meta: {chunk['meta']}")

                # í•œêµ­ì–´ í¬í•¨ ì—¬ë¶€ í™•ì¸
                korean_chars = len(re.findall(r'[ê°€-í£]', chunk['content']))
                print(f"Korean characters: {korean_chars}")

        except Exception as e:
            print(f"âŒ Failed: {e}")
            import traceback
            traceback.print_exc()
    else:
        print("\nğŸ’¡ Usage: python parser.py <pdf_file_path>")